{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a703448",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from layerwise_ntk import compute_NTK_CNN\n",
    "import numpy as np\n",
    "import random\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch import load\n",
    "from torch.nn import functional as F\n",
    "from torch import autograd\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import time\n",
    "\n",
    "from numba import njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07c5caeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = torch.tensor([0.0])\n",
    "#a = a.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "774508f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "how_many = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1659e5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    return torch.tanh(x)\n",
    "\n",
    "@njit\n",
    "def d_activation(x):\n",
    "    return np.cosh(x)**-2\n",
    "\n",
    "def activation(x):\n",
    "    return torch.relu(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dded7b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NTK_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        print(m.weight.shape)\n",
    "        nn.init.normal_(m.weight.data)#/m.weight.shape[0]\n",
    "        if m.bias != None:\n",
    "            nn.init.normal_(m.bias.data)#/m.weight.shape[0]\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        print(m.weight.shape)\n",
    "        nn.init.normal_(m.weight.data)#/m.weight.shape[0]\n",
    "        if m.bias != None:\n",
    "            nn.init.normal_(m.bias.data)#/m.weight.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "188869b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dumb_small_layerwise(torch.nn.Module):\n",
    "    '''\n",
    "    simple network for test cases\n",
    "    \n",
    "    \n",
    "    It seems like bias vectors aren't trivially added.\n",
    "    '''\n",
    "    def __init__(self,):\n",
    "        super(dumb_small_layerwise, self).__init__()\n",
    "        \n",
    "        self.d1 = torch.nn.Linear(1,1,bias=True) #28 -> 28\n",
    "\n",
    "        self.d2 = torch.nn.Linear(1,1,3,stride=1,padding=1,bias=True) #28 -> 28\n",
    "        \n",
    "        self.d3 = torch.nn.Linear(1,1,3,stride=1,padding=1,bias=True) #28 -> 28\n",
    "        \n",
    "        self.d4 = torch.nn.Linear(1,1,3,stride=1,padding=1,bias=True) #28 -> 28\n",
    "        \n",
    "        self.d5 = torch.nn.Linear(1*28*28,1,bias=True)\n",
    "        \n",
    "    def forward(self, x_0):\n",
    "        x_1 = activation(self.d1(x_0))\n",
    "        x_2 = activation(self.d2(x_1))\n",
    "        x_3 = activation(self.d3(x_2))\n",
    "        x_4 = activation(self.d4(x_3))\n",
    "        x_5 = x_4.reshape(how_many,-1)\n",
    "        x_6 = activation(self.d5(x_5))\n",
    "        return x_6, x_5, x_4, x_3, x_2, x_1, x_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d39fc4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 3, 3])\n",
      "torch.Size([1, 1, 3, 3])\n",
      "torch.Size([1, 1, 3, 3])\n",
      "torch.Size([1, 1, 3, 3])\n",
      "torch.Size([1, 784])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "device='cuda'\n",
    "\n",
    "model = dumb_small_layerwise()\n",
    "model.apply(NTK_weights)\n",
    "model.to(device)\n",
    "\n",
    "x_test = np.random.normal(0,1,(how_many,1,28,28)).astype(np.float32) #n c_in, h, w\n",
    "x_test = torch.from_numpy(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5d26f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test.to('cuda')\n",
    "x_6, x_5, x_4, x_3, x_2, x_1, x_0 = model(x_test)\n",
    "\n",
    "#These need to be numpy\n",
    "Ws = []\n",
    "Ws.append(torch.tensor([0.0],dtype=torch.float32)) \n",
    "Ws.append(torch.tensor([0.0],dtype=torch.float32)) \n",
    "Ws.append(torch.tensor([0.0],dtype=torch.float32))\n",
    "Ws.append(torch.tensor([0.0],dtype=torch.float32)) #spacer\n",
    "Ws.append(torch.tensor([0.0],dtype=torch.float32))\n",
    "Ws.append(model.d5.weight.detach())\n",
    "\n",
    "#Kernel Matrices, Need to be numpy\n",
    "Ks = []\n",
    "Ks.append(model.d1.weight.detach())\n",
    "Ks.append(model.d2.weight.detach())\n",
    "Ks.append(model.d3.weight.detach())\n",
    "Ks.append(model.d4.weight.detach())\n",
    "Ks.append(torch.tensor([0.0],dtype=torch.float32)) #spacer\n",
    "Ks.append(torch.tensor([0.0],dtype=torch.float32))\n",
    "\n",
    "\n",
    "Xs = [] # Xs are shape (output x #DP) ; however, typical python notation is reversed, so we take transpose here\n",
    "Xs.append(x_0.T.detach())\n",
    "Xs.append(x_1.T.detach())\n",
    "Xs.append(x_2.T.detach())\n",
    "Xs.append(x_3.T.detach())\n",
    "Xs.append(x_4.T.detach())\n",
    "Xs.append(x_5.T.detach())\n",
    "\n",
    "#This is used to create arrays-- needs to be integer list to play nice with compilers\n",
    "ds_int = []\n",
    "ds_int.append(1*3*3) #channels_out * kernel_height * kernel_width\n",
    "ds_int.append(1*3*3) #channels_out * kernel_height * kernel_width\n",
    "ds_int.append(1*3*3) #channels_out * kernel_height * kernel_width\n",
    "ds_int.append(1*3*3) #channels_out * kernel_height * kernel_width\n",
    "ds_int.append(1) #channels_out * kernel_height * kernel_width\n",
    "ds_int.append(1) #channels_out * kernel_height * kernel_width\n",
    "\n",
    "ds_array = [] #this is for the NTK formulation, \n",
    "#ds_array.append(torch.tensor([1.0],dtype=torch.float32).to(device)) #first element is a spacer, could be anything.\n",
    "\n",
    "ds_array.append(torch.tensor([1.0],dtype=torch.float32).to(device)) #first element is a spacer, could be anything.\n",
    "ds_array.append(torch.tensor([1.0],dtype=torch.float32).to(device)) #The rest, even if you dont use NTK formulation, would be 1\n",
    "ds_array.append(torch.tensor([1.0],dtype=torch.float32).to(device))\n",
    "ds_array.append(torch.tensor([1.0],dtype=torch.float32).to(device))\n",
    "ds_array.append(torch.tensor([1.0],dtype=torch.float32).to(device))\n",
    "ds_array.append(torch.tensor([1.0],dtype=torch.float32).to(device))\n",
    "\n",
    "filters = []\n",
    "filters.append(1)\n",
    "filters.append(1)\n",
    "filters.append(1)\n",
    "filters.append(1)\n",
    "filters.append(0)\n",
    "filters.append(0)\n",
    "\n",
    "\n",
    "padding = []\n",
    "padding.append(1)\n",
    "padding.append(1)\n",
    "padding.append(1)\n",
    "padding.append(1)\n",
    "padding.append(0)\n",
    "padding.append(0)\n",
    "\n",
    "\n",
    "strides = []\n",
    "strides.append(1)\n",
    "strides.append(1)\n",
    "strides.append(1)\n",
    "strides.append(1)\n",
    "strides.append(0)\n",
    "strides.append(0)\n",
    "\n",
    "\n",
    "layers=[model.d1,\n",
    "        model.d2,\n",
    "        model.d3,\n",
    "        model.d4,\n",
    "        0.0,\n",
    "        model.d5\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96ada7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_activationt(x):\n",
    "    x[x>0.0]=1.0\n",
    "    x[x<0.0]=0.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84918700",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def calc_dw(x,w,b,pad,stride,H_,W_):\n",
    "    \"\"\"\n",
    "    Calculates the derivative of conv(x,w) with respect to w\n",
    "    \n",
    "    output is shape:\n",
    "        [datapoints, in_channels out_filters, kernel_height, kernel_width, out_filters, data_height, data_width\n",
    "    \n",
    "    'n f1 f2 c kh kw dh dw -> n (c f1 kh kw) (f2 dh dw)'\n",
    "    \"\"\"\n",
    "    dx, dw, db = None, None, None\n",
    "    N, C, H, W = x.shape\n",
    "    F, _, HH, WW = w.shape\n",
    "    \n",
    "    #dw = np.zeros((N,F,F,C,HH,WW,H_,W_),dtype=np.float32)\n",
    "    \n",
    "    dw = np.zeros((N,C,F,HH,WW,F,H_,W_),dtype=np.float32)\n",
    "\n",
    "    xp = zero_pad(x,pad)\n",
    "    #high priority, how to vectorize this operation?\n",
    "    for n in range(N):\n",
    "        for f in range(F):\n",
    "            for i in range(HH): \n",
    "                for j in range(WW): \n",
    "                    for k in range(H_): \n",
    "                        for l in range(W_): \n",
    "                            for c in range(C): \n",
    "                                dw[n,c,f,i,j,f,k,l] += xp[n, c, i+stride*k, j+stride*l]                             \n",
    "    \n",
    "    return dw.reshape((N,(C*F*HH*WW),(F*H_*W_)))\n",
    "\n",
    "@njit\n",
    "def calc_dx(x,w,b,pad,stride,H_,W_):\n",
    "    '''\n",
    "    calculates the derivative of conv(x,w) with respect to x\n",
    "    \n",
    "    output is a nd-array of shape n x ch_in x og_h x og_w x (h_out w_out ch_out)\n",
    "    '''\n",
    "    dx, dw, db = None, None, None\n",
    "    N, C, H, W = x.shape\n",
    "    F, _, HH, WW = w.shape \n",
    "\n",
    "    dx = np.zeros((C,H,W,F,H_,W_,),dtype=np.float32)\n",
    "    #high priority, how to vectorize this operation? maybe with np.chunk,split?\n",
    "    for f in range(F): \n",
    "        for i in range(H): \n",
    "            for j in range(W):\n",
    "                for k in range(H_): \n",
    "                    for l in range(W_):\n",
    "                        for c in range(C): \n",
    "                            if i-stride*k+pad > HH-1 or j-stride*l+pad > WW-1:\n",
    "                                continue #this is alternative to padding w with zeros.\n",
    "                            if i-stride*k+pad < 0 or j-stride*l+pad < 0:\n",
    "                                continue #this is alternative to padding w with zeros.\n",
    "                            dx[c,i,j,f,k,l] += w[f, c, i-stride*k+pad, j-stride*l+pad]\n",
    "    #'c ih iw f oh ow -> (c ih iw) (f oh ow)'\n",
    "    return dx.reshape(((C*H*W),(F*H_*W_)))\n",
    "    \n",
    "@njit\n",
    "def zero_pad(A,pad):\n",
    "    N, F, H, W = A.shape\n",
    "    P = np.zeros((N, F, H+2*pad, W+2*pad),dtype=np.float32)\n",
    "    P[:,:,pad:H+pad,pad:W+pad] = A\n",
    "    return P\n",
    "    \n",
    "@njit\n",
    "def cross(X):\n",
    "    return X.T.dot(X)\n",
    "\n",
    "def cross_pt_nonp(X,device='cuda'):\n",
    "    X = X.to(device)\n",
    "    return X.T.matmul(X)\n",
    "\n",
    "def cross_pt(X,device='cuda'):\n",
    "    X = torch.from_numpy(X).to(device)\n",
    "    X = X.to(device)\n",
    "    return X.T.matmul(X).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd6779ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_NTK_CNN(Ws: list, Ks: list, Xs: list, d_int: list, d_array: list, strides: list, padding: list, layers: list, d_activationt, device=\"cuda\", ) -> list:\n",
    "    '''\n",
    "    MAIN:\n",
    "    \n",
    "    Inputs: \n",
    "    Ws: list, has length number of layers + any reshaping, contains dense layer weight tensors, detatched, and on device\n",
    "    Ks: list, has length number of layers + any reshaping, contains 2d convolutional layers weight tensors, detatched, and on device\n",
    "    Xs: list, has length number of layers + any reshaping, contains all intermediate outputs of each layer in the models\n",
    "    d_int: list, has the number of bias parameters in each dense layer in the models\n",
    "    d_array: list, has the value of which to sqrt and divide by in each layer, typically called the NTK normalization. else, its values are 1.\n",
    "    strides: list, has the value of stride in each convolutional layer, else 0\n",
    "    padding: list, has the value of padding in each convolutional layer, else 0\n",
    "    layers: list, is a list containing the pytorch layers in the model, and \"0\" as a placeholder for a reshaping layer\n",
    "    device: str, one of either 'cpu' or 'cuda'; must be the same as the model device location\n",
    "    \n",
    "    NOTE: all of the above lists should have the same length! See example\n",
    "    \n",
    "    OUTPUTS: list of torch.tensor objects, each the ntk 'component' for that layer. Given in backwards order, i.e. starting with the last layer. First weight, then bias.\n",
    "    \n",
    "    NOTE: to get the full ntk, simply sum over the layer dimension of the result: NTK = torch.sum(torch.stack(components),dim=(0))\n",
    "    '''\n",
    "    components = []\n",
    "    \n",
    "    L = len(Xs)-1 #number of layers, Xs goes from inputs to right before outputs; X_0 is the input, X_L CK\n",
    "    n = Xs[0].shape[-1] #number of datapoints\n",
    "\n",
    "    #holds the derivatives of activation, first value is empty list...?; just a spacer, replace with array\n",
    "    Ds_dense = [np.array([[0.0]],dtype=np.float32)] \n",
    "    Ds_conv = [np.array([[0.0]],dtype=np.float32)]\n",
    "    s_matrices = []\n",
    "    with torch.no_grad():\n",
    "        ####################################################################################################\n",
    "        for l in range(0,L):\n",
    "            if isinstance(layers[l],torch.nn.Linear):\n",
    "                Ds_dense.append(d_activationt(layers[l](Xs[l].T)).T)\n",
    "            else:\n",
    "                Ds_dense.append(np.array([[0.0]],dtype=np.float32))\n",
    "        ################################################################################################\n",
    "        for l in range(0,L):\n",
    "            if isinstance(layers[l],torch.nn.Conv2d):\n",
    "                Ds_conv.append(d_activationt(layers[l](Xs[l].T)).reshape(n,-1).T)\n",
    "            else:\n",
    "                Ds_conv.append(np.array([[0.0]],dtype=np.float32))      \n",
    "        ####################################################################################################\n",
    "        S = torch.tensor([1.0],dtype=torch.float32).to(device) #this models the backward propogation:   \n",
    "        for l in range(L,-1,-1):\n",
    "            if isinstance(layers[l], torch.nn.Linear):\n",
    "\n",
    "                components.append(cross_pt_nonp(S,device)*cross_pt_nonp(Xs[l],device)/d_array[l])\n",
    "\n",
    "                W = torch.ones((d_int[l],n),dtype=torch.float32).to(device) * S\n",
    "                components.append(cross_pt_nonp(W,device).to(device)/d_array[l])\n",
    "\n",
    "            elif isinstance(layers[l], torch.nn.Conv2d):\n",
    "                if len(S.shape) == 2: #this should only affect the very last layer, at which point, who cares.\n",
    "                    S = S[None,:,:]\n",
    "                    \n",
    "                #print('l: ',l)\n",
    "                #print('S shape: ',S.shape)\n",
    "                #print('frac of non-zero values in S: ',torch.sum(S!=0)/len(S.reshape(-1)))\n",
    "                dw = calc_dw(x=Xs[l].T.cpu().numpy(),w=Ks[l].cpu().numpy(),b=0,pad=padding[l],stride=strides[l],H_=Xs[l+1].shape[1],W_=Xs[l+1].shape[0])\n",
    "                \n",
    "                #print('dw shape: ',dw.shape)\n",
    "                #dw = dw[:,:,::filters[l]] #gets rid of the nonzero elements\n",
    "                #S = S[:,::filters[l],:]\n",
    "                \n",
    "                W = torch.matmul(torch.from_numpy(dw).to(device),S.to(device))\n",
    "                \n",
    "                #We should bring this to zhichao or sombody and ask if there is obviously something faster?\n",
    "                #W = np.diagonal(W,0,2,0)\n",
    "                #print('W b4 diagonal: ',W.shape)\n",
    "                W = torch.diagonal(W,0,0,2)\n",
    "                #print('W final shape: ',W.shape)\n",
    "                #print(' ')\n",
    "                components.append(cross_pt_nonp(W,device).to(device)/d_array[l])\n",
    "                #print(components[-1])\n",
    "\n",
    "                N = Ks[l].shape[0]\n",
    "                W = np.split(S.cpu().numpy(),N,axis=1)\n",
    "                #W = torch.split(S,N,dim=1)\n",
    "                \n",
    "                W = np.array(W)\n",
    "                #W = torch.stack(W)\n",
    "                \n",
    "                W = np.sum(W,axis=(1,2))\n",
    "                #W = torch.sum(W,dim=(1,2))\n",
    "                \n",
    "                components.append(torch.from_numpy(cross(W,)).to(device)/d_array[l])\n",
    "                #components.append(cross_pt_nonp(W,device))\n",
    "\n",
    "            #############################\n",
    "            #now we setup S for the next loop by treating appropriately\n",
    "            if l==0:\n",
    "                break\n",
    "\n",
    "            if isinstance(layers[l], torch.nn.Linear):\n",
    "                S = torch.matmul(S.T,Ws[l]) / torch.sqrt(d_array[l])\n",
    "                if len(S.shape) < 2:\n",
    "                    S = S[:,None] #expand dimension along axis 1\n",
    "                if not(isinstance(layers[l-1],float)): #this exludes the reshaping layer\n",
    "                    S = Ds_dense[l]*S\n",
    "                else: #and when the reshaping layer occurs we need to apply this instead\n",
    "                    S = Ds_conv[l-1]*S\n",
    "\n",
    "            elif isinstance(layers[l], torch.nn.Conv2d):\n",
    "                dx = calc_dx(x=Xs[l].T.cpu().numpy(),w=Ks[l].cpu().numpy(),b=0,pad=padding[l],stride=strides[l],H_=Xs[l+1].shape[1],W_=Xs[l+1].shape[0])\n",
    "                S = (torch.from_numpy(dx[None,:,:]).to(device) @ S) / torch.sqrt(d_array[l])\n",
    "                S = Ds_conv[l]*S\n",
    "            \n",
    "    return components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04fa9409",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 292.06 GiB (GPU 0; 11.91 GiB total capacity; 1.42 GiB already allocated; 9.38 GiB free; 1.65 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15075/264595.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mntk_components\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_NTK_CNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_activationt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_15075/2906603716.py\u001b[0m in \u001b[0;36mcompute_NTK_CNN\u001b[0;34m(Ws, Ks, Xs, d_int, d_array, strides, padding, layers, d_activationt, device)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0;31m#S = S[:,::filters[l],:]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;31m#We should bring this to zhichao or sombody and ask if there is obviously something faster?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 292.06 GiB (GPU 0; 11.91 GiB total capacity; 1.42 GiB already allocated; 9.38 GiB free; 1.65 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "ntk_components = compute_NTK_CNN(Ws, Ks, Xs, ds_int, ds_array, strides, padding, layers, d_activationt, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2239f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "ntk_components = compute_NTK_CNN(Ws, Ks, Xs, ds_int, ds_array, strides, padding, layers, d_activationt, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a37a7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "NTK = torch.sum(torch.stack(ntk_components),[0,])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
