{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook demonstrates that the various methods all agree with one another, and demonstrates how to setup the calculation correctly for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch import load\n",
    "from torch.nn import functional as F\n",
    "from torch import autograd\n",
    "\n",
    "from torchvision import datasets\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import time\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from numba import njit\n",
    "\n",
    "import os\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchntk.autograd import naive_ntk\n",
    "from torchntk.autograd import old_autograd_ntk\n",
    "from torchntk.explicit import explicit_ntk\n",
    "from torchntk.autograd import vmap_ntk_loader\n",
    "from torchntk.autograd import autograd_components_ntk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "HOW_MANY = 3 #just to demonstrate different methods, confirm same answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    return torch.tanh(x)\n",
    "\n",
    "def d_activationt(x):\n",
    "    return torch.cosh(x)**-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NTK_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        print(m.weight.shape)\n",
    "        nn.init.normal_(m.weight.data)#/m.weight.shape[0]\n",
    "        if m.bias != None:\n",
    "            nn.init.normal_(m.bias.data)#/m.weight.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need two different, but similar, architectures\n",
    "\n",
    "* FC_layered, which we use for the explicit, returns all intermediate layer outputs which we will use in our calculation\n",
    "* FC, which has a typical return of just the single output neuron of the network\n",
    "\n",
    "\n",
    "We will use seed to set the weights, s.t. we can gurantee our two networks are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC_layered(torch.nn.Module):\n",
    "    '''\n",
    "    simple network for test cases\n",
    "    \n",
    "    \n",
    "    It seems like bias vectors aren't trivially added.\n",
    "    '''\n",
    "    def __init__(self,):\n",
    "        super(FC_layered, self).__init__()\n",
    "        #input size=(N,784)\n",
    "        self.d1 = torch.nn.Linear(100,10,bias=False)\n",
    "\n",
    "        self.d2 = torch.nn.Linear(10,100,bias=False)\n",
    "        \n",
    "        self.d3 = torch.nn.Linear(100,10,bias=False)\n",
    "        \n",
    "        self.d4 = torch.nn.Linear(10,1,bias=False)\n",
    "        \n",
    "    def forward(self, x0):\n",
    "        x1 = 1/np.sqrt(10) * activation(self.d1(x0))\n",
    "        x2 = 1/np.sqrt(100) * activation(self.d2(x1))\n",
    "        x3 = 1/np.sqrt(10) * activation(self.d3(x2))\n",
    "        x4 = self.d4(x3)\n",
    "        return x4, x3, x2, x1, x0 #NOTICE return all intermediate outputs \n",
    "    \n",
    "class FC(torch.nn.Module):\n",
    "    '''\n",
    "    simple network for test cases\n",
    "    \n",
    "    \n",
    "    It seems like bias vectors aren't trivially added.\n",
    "    '''\n",
    "    def __init__(self,):\n",
    "        super(FC, self).__init__()\n",
    "        #input size=(N,784)\n",
    "        self.d1 = torch.nn.Linear(100,10,bias=False)\n",
    "\n",
    "        self.d2 = torch.nn.Linear(10,100,bias=False)\n",
    "        \n",
    "        self.d3 = torch.nn.Linear(100,10,bias=False)\n",
    "        \n",
    "        self.d4 = torch.nn.Linear(10,1,bias=False)\n",
    "        \n",
    "    def forward(self, x0):\n",
    "        x1 = 1/np.sqrt(10) * activation(self.d1(x0))\n",
    "        x2 = 1/np.sqrt(100) * activation(self.d2(x1))\n",
    "        x3 = 1/np.sqrt(10) * activation(self.d3(x2))\n",
    "        x4 = self.d4(x3)\n",
    "        return x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 100])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([100, 10])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([1, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FC_layered(\n",
       "  (d1): Linear(in_features=100, out_features=10, bias=False)\n",
       "  (d2): Linear(in_features=10, out_features=100, bias=False)\n",
       "  (d3): Linear(in_features=100, out_features=10, bias=False)\n",
       "  (d4): Linear(in_features=10, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "device='cuda'\n",
    "\n",
    "model = FC()\n",
    "model.to(device)\n",
    "model.apply(NTK_weights)\n",
    "####\n",
    "\n",
    "#layerwise depends on a special nn.Module object\n",
    "#that returns the tuple over each layer's output\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "model2 = FC_layered()\n",
    "model2.to(device)\n",
    "model2.apply(NTK_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now setup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "train_x = torch.empty((HOW_MANY,100),device='cpu').normal_(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch.Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "if HOW_MANY <= 1000: #takes a long time\n",
    "    NTK_naive = naive_ntk(model.cpu(),train_x_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTK old Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FC(\n",
       "  (d1): Linear(in_features=100, out_features=10, bias=False)\n",
       "  (d2): Linear(in_features=10, out_features=100, bias=False)\n",
       "  (d3): Linear(in_features=100, out_features=10, bias=False)\n",
       "  (d4): Linear(in_features=10, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgts = torch.zeros(HOW_MANY,device='cpu')\n",
    "xloader = DataLoader(TensorDataset(train_x,tgts),batch_size=128,pin_memory=True,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "NTK_old_autograd = old_autograd_ntk(xloader,model,device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTK layerwise Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(train_x.to(device))\n",
    "NTK_autograd_components = autograd_components_ntk(model,y[:,0])\n",
    "NTK_autograd_full = torch.sum(torch.stack([val for val in NTK_autograd_components.values()]),dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTK layerwise by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(model, x_test):\n",
    "    x_4, x_3, x_2, x_1, x_0 = model(x_test)\n",
    "\n",
    "\n",
    "    Xs = [] # Xs are shape (output x #DP) ; however, typical python notation is reversed, so we take transpose here\n",
    "    Xs.append(x_0.T.detach())\n",
    "    Xs.append(x_1.T.detach())\n",
    "    Xs.append(x_2.T.detach())\n",
    "    Xs.append(x_3.T.detach())\n",
    "\n",
    "    #This is used to create arrays-- needs to be integer list to play nice with compilers\n",
    "    d_int = []\n",
    "    d_int.append(10)\n",
    "    d_int.append(100)\n",
    "    d_int.append(10)\n",
    "    d_int.append(1)\n",
    "\n",
    "    d_array = [] #this is for the NTK formulation, \n",
    "    #ds_array.append(torch.tensor([1.0],dtype=torch.float32).to(device)) #first element is a spacer, could be anything.\n",
    "\n",
    "    d_array.append(torch.tensor([10.0],dtype=torch.float32).to(device)) \n",
    "    d_array.append(torch.tensor([100.0],dtype=torch.float32).to(device)) \n",
    "    d_array.append(torch.tensor([10.0],dtype=torch.float32).to(device))\n",
    "    d_array.append(torch.tensor([1.0],dtype=torch.float32).to(device))\n",
    "\n",
    "    layers=[model.d1,\n",
    "            model.d2,\n",
    "            model.d3,\n",
    "            model.d4,\n",
    "           ]\n",
    "    \n",
    "    #Ws Ks Xs d_int d_array strides padding layers d_activationt, device=\"cuda\"\n",
    "    return x_4[:,0], {'Xs':Xs, 'd_int':d_int, 'd_array':d_array, 'layers':layers, 'd_activationt':d_activationt, 'device':'cuda'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.zero_grad()\n",
    "\n",
    "output, params = forward(model2, train_x.to(device))\n",
    "\n",
    "NTK_explicit_components = explicit_ntk(**params)\n",
    "NTK_explicit_full = torch.sum(torch.stack(NTK_explicit_components),dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTK with Vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "NTK_vmap_components = vmap_ntk_loader(model,xloader)\n",
    "NTK_vmap_full = torch.sum(torch.stack([val for val in NTK_vmap_components.values()]),dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare full NTKs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.8789, -0.0399, -0.1851],\n",
       "        [-0.0399,  0.5906,  0.3424],\n",
       "        [-0.1851,  0.3424,  2.0714]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NTK_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.8831, -0.0398, -0.1852],\n",
       "        [-0.0398,  0.5904,  0.3423],\n",
       "        [-0.1852,  0.3423,  2.0716]], device='cuda:0')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NTK_old_autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.8831, -0.0398, -0.1852],\n",
       "        [-0.0398,  0.5904,  0.3423],\n",
       "        [-0.1852,  0.3423,  2.0716]], device='cuda:0')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NTK_autograd_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.8781, -0.0398, -0.1851],\n",
       "        [-0.0398,  0.5903,  0.3423],\n",
       "        [-0.1851,  0.3423,  2.0708]], device='cuda:0')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NTK_explicit_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.8831, -0.0398, -0.1852],\n",
       "        [-0.0398,  0.5904,  0.3423],\n",
       "        [-0.1852,  0.3423,  2.0716]], device='cuda:0')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NTK_vmap_full"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
