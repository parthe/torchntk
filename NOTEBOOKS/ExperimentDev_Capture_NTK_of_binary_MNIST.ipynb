{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4380e90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch import load\n",
    "from torch.nn import functional as F\n",
    "from torch import autograd\n",
    "\n",
    "from torchvision import datasets\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import time\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from numba import njit\n",
    "\n",
    "import os\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd216249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.vmap is currently available only on nightly releases: torch version  1.11.0.dev20220127\n"
     ]
    }
   ],
   "source": [
    "from torchntk.explicit import explicit_ntk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02f9ea41",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_base_path = './experiments/'\n",
    "\n",
    "d1_path = os.path.join(experiment_base_path,'d1_component')\n",
    "d2_path = os.path.join(experiment_base_path,'d2_component')\n",
    "d3_path = os.path.join(experiment_base_path,'d3_component')\n",
    "d4_path = os.path.join(experiment_base_path,'d4_component')\n",
    "\n",
    "ntk_path = os.path.join(experiment_base_path,'ntk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb1a4c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(d1_path,exist_ok=True)\n",
    "os.makedirs(d2_path,exist_ok=True)\n",
    "os.makedirs(d3_path,exist_ok=True)\n",
    "os.makedirs(d4_path,exist_ok=True)\n",
    "os.makedirs(ntk_path,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7be8687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "N_EPOCHS=100\n",
    "LR=1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cf0076a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    return torch.tanh(x)\n",
    "\n",
    "def d_activationt(x):\n",
    "    return torch.cosh(x)**-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28847a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NTK_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        print(m.weight.shape)\n",
    "        nn.init.normal_(m.weight.data)#/m.weight.shape[0]\n",
    "        if m.bias != None:\n",
    "            nn.init.normal_(m.bias.data)#/m.weight.shape[0]\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        print(m.weight.shape)\n",
    "        nn.init.normal_(m.weight.data)#/m.weight.shape[0]\n",
    "        if m.bias != None:\n",
    "            nn.init.normal_(m.bias.data)#/m.weight.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a1ca5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC(torch.nn.Module):\n",
    "    '''\n",
    "    simple network for test cases\n",
    "    \n",
    "    \n",
    "    It seems like bias vectors aren't trivially added.\n",
    "    '''\n",
    "    def __init__(self,):\n",
    "        super(FC, self).__init__()\n",
    "        #input size=(N,784)\n",
    "        self.d1 = torch.nn.Linear(784,100,bias=False)\n",
    "\n",
    "        self.d2 = torch.nn.Linear(100,100,bias=False)\n",
    "        \n",
    "        self.d3 = torch.nn.Linear(100,100,bias=False)\n",
    "        \n",
    "        self.d4 = torch.nn.Linear(100,1,bias=False)\n",
    "        \n",
    "    def forward(self, x0):\n",
    "        x1 = 1/np.sqrt(100) * activation(self.d1(x0))\n",
    "        x2 = 1/np.sqrt(100) * activation(self.d2(x1))\n",
    "        x3 = 1/np.sqrt(100) * activation(self.d3(x2))\n",
    "        x4 = self.d4(x3)\n",
    "        return x4, x3, x2, x1, x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "106f6d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 784])\n",
      "torch.Size([100, 100])\n",
      "torch.Size([100, 100])\n",
      "torch.Size([1, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FC(\n",
       "  (d1): Linear(in_features=784, out_features=100, bias=False)\n",
       "  (d2): Linear(in_features=100, out_features=100, bias=False)\n",
       "  (d3): Linear(in_features=100, out_features=100, bias=False)\n",
       "  (d4): Linear(in_features=100, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "device='cuda'\n",
    "\n",
    "model = FC()\n",
    "model.to(device)\n",
    "model.apply(NTK_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a44c578b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(\n",
    "    root = './DATA/',\n",
    "    train = True,                          \n",
    "    download = True,            \n",
    ")\n",
    "\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root = './DATA/', \n",
    "    train = False, \n",
    "    download=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bca5ebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_data.data\n",
    "test_x = test_data.data\n",
    "\n",
    "train_y = train_data.targets\n",
    "test_y = test_data.targets\n",
    "\n",
    "train_mask = torch.logical_or(train_y==6,train_y==9)\n",
    "test_mask = torch.logical_or(test_y==6,test_y==9)\n",
    "\n",
    "train_x = train_x[train_mask]\n",
    "train_y = train_y[train_mask]\n",
    "\n",
    "test_x = test_x[test_mask]\n",
    "test_y = test_y[test_mask]\n",
    "\n",
    "train_x = train_x/255.0\n",
    "test_x = test_x/255.0\n",
    "\n",
    "train_y[train_y==6] = 0\n",
    "train_y[train_y==9] = 1\n",
    "\n",
    "test_y[test_y==6] = 0\n",
    "test_y[test_y==9] = 1\n",
    "\n",
    "train_x = train_x.reshape(-1,784)\n",
    "test_x = test_x.reshape(-1,784)\n",
    "\n",
    "train_y = train_y.float()\n",
    "test_y = test_y.float()\n",
    "\n",
    "train_x = train_x - torch.mean(train_x)\n",
    "train_x = train_x / torch.std(train_x)\n",
    "\n",
    "test_x = test_x - torch.mean(train_x)\n",
    "test_x = test_x / torch.std(test_x)\n",
    "\n",
    "mask_6 = train_y == 0\n",
    "mask_9 = train_y == 1\n",
    "\n",
    "train_x_6 = train_x[mask_6]\n",
    "train_x_9 = train_x[mask_9]\n",
    "\n",
    "train_y_6 = train_y[mask_6]\n",
    "train_y_9 = train_y[mask_9]\n",
    "\n",
    "mask_6 = test_y==0\n",
    "mask_9 = test_y==1\n",
    "\n",
    "test_x_6 = test_x[mask_6]\n",
    "test_x_9 = test_x[mask_9]\n",
    "\n",
    "test_y_6 = test_y[mask_6]\n",
    "test_y_9 = test_y[mask_9]\n",
    "\n",
    "#grab 2000 from each, turn that back into train x and train y, which is now sorted.\n",
    "\n",
    "train_y = torch.cat([train_y_6[0:5000],train_y_9[0:5000]])\n",
    "train_x = torch.cat([train_x_6[0:5000],train_x_9[0:5000]])\n",
    "\n",
    "test_x = torch.cat([test_x_6,test_x_9])\n",
    "test_y = torch.cat([test_y_6,test_y_9])\n",
    "\n",
    "train_x = train_x.to('cuda')\n",
    "train_y = train_y.to('cuda')\n",
    "\n",
    "test_x = test_x.to('cuda')\n",
    "test_y = test_y.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b32bb63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ed61820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(model, x_test):\n",
    "    x_4, x_3, x_2, x_1, x_0 = model(x_test)\n",
    "\n",
    "    Xs = [] # Xs are shape (output x #DP) ; however, typical python notation is reversed, so we take transpose here\n",
    "    Xs.append(x_0.T.detach())\n",
    "    Xs.append(x_1.T.detach())\n",
    "    Xs.append(x_2.T.detach())\n",
    "    Xs.append(x_3.T.detach())\n",
    "\n",
    "    #This is used to create arrays-- needs to be integer list to play nice with compilers\n",
    "    d_int = []\n",
    "    d_int.append(100)\n",
    "    d_int.append(100)\n",
    "    d_int.append(100)\n",
    "    d_int.append(1)\n",
    "\n",
    "    d_array = [] #this is for the NTK formulation, \n",
    "    #ds_array.append(torch.tensor([1.0],dtype=torch.float32).to(device)) #first element is a spacer, could be anything.\n",
    "\n",
    "    d_array.append(torch.tensor([100.0],dtype=torch.float32).to(device)) \n",
    "    d_array.append(torch.tensor([100.0],dtype=torch.float32).to(device)) \n",
    "    d_array.append(torch.tensor([100.0],dtype=torch.float32).to(device))\n",
    "    d_array.append(torch.tensor([1.0],dtype=torch.float32).to(device))\n",
    "\n",
    "    layers=[model.d1,\n",
    "            model.d2,\n",
    "            model.d3,\n",
    "            model.d4,\n",
    "           ]\n",
    "    \n",
    "    #Ws Ks Xs d_int d_array strides padding layers d_activationt, device=\"cuda\"\n",
    "    return x_4[:,0], {'Xs':Xs, 'd_int':d_int, 'd_array':d_array, 'layers':layers, 'd_activationt':d_activationt, 'device':'cuda'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a57d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_spectrum(NTK_component,epoch,save_str):\n",
    "    '''\n",
    "    takes a numpy array and saves the image to save_str\n",
    "    '''\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    plt.hist(torch.linalg.eigvalsh(NTK_component).detach().cpu().numpy(), bins=np.logspace(-14,4,100,10.0))\n",
    "    med = torch.mean(NTK_component)\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('eigenvalue')\n",
    "    plt.title('MNIST-2 {} eigenvalue spectrum, step={}'.format(save_str,str(epoch).zfill(5)))\n",
    "    plt.vlines(med.detach().cpu().numpy(),0,200,color='k',linestyle='dashed')\n",
    "    #plt.savefig(os.path.join(experiment_base_path,'images',save_str,str(epoch).zfill(5)+'.png'))\n",
    "    plt.savefig(os.path.join('./images','spectrumMNIST2'+save_str+'_'+str(epoch).zfill(5)+'.pdf'))\n",
    "    plt.close()\n",
    "    #plt.show()\n",
    "    \n",
    "def visualize_spectrum_linear(NTK_component,epoch,save_str):\n",
    "    '''\n",
    "    takes a numpy array and saves the image to save_str\n",
    "    '''\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    plt.hist(torch.linalg.eigvalsh(NTK_component).detach().cpu().numpy(), bins=np.linspace(1e-14,1e4,100,10.0))\n",
    "    plt.xlabel('eigenvalue')\n",
    "    plt.title('{} eigenvalue spectrum: {}'.format(save_str,str(epoch).zfill(5)))\n",
    "    plt.savefig(os.path.join(experiment_base_path,'images',save_str,str(epoch).zfill(5)+'.png'))\n",
    "    plt.close()\n",
    "    #plt.show()\n",
    "    \n",
    "def visualize_ntk_matrix(NTK_component,epoch,save_str):\n",
    "    '''\n",
    "    plots a heatmap of the NTK, and does so without changing the values\n",
    "    '''\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    plt.imshow(NTK_component.detach().cpu().numpy(),cmap='Greys')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('NTK index j')\n",
    "    plt.ylabel('NTK index i')\n",
    "    plt.title('{} matrix map: {}'.format(save_str,str(epoch).zfill(5)))\n",
    "    plt.show()\n",
    "    #plt.savefig(os.path.join(experiment_base_path,'images_matrixmap',save_str,str(epoch).zfill(5)+'.png'))\n",
    "    #plt.close()\n",
    "    \n",
    "def visualize_ntk_matrix_scaled(NTK_component,epoch,save_str):\n",
    "    '''\n",
    "    plots a heatmap of the NTK, and does so by scaling the data first to be between -1 and 1\n",
    "    '''\n",
    "    NTK_component = NTK_component.detach().cpu().numpy()\n",
    "    #scale between 0, 1\n",
    "    vmin, vmax = np.quantile(NTK_component,[0.01,0.99])\n",
    "    #\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    plt.imshow(NTK_component,cmap='Greys',vmin=vmin, vmax=vmax)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('NTK index j')\n",
    "    plt.ylabel('NTK index i')\n",
    "    plt.title('MNIST-2 {} matrix map, step={}'.format(save_str,str(epoch).zfill(5)))\n",
    "    \n",
    "    #plt.show()\n",
    "    #plt.savefig(os.path.join(experiment_base_path,'images_matrixmapscaled',save_str,str(epoch).zfill(5)+'.png'))\n",
    "    plt.savefig(os.path.join('./images','matrixMNIST2_rel_'+save_str+'_'+str(epoch).zfill(5)+'.pdf'))\n",
    "    plt.close()\n",
    "    \n",
    "def visualize_ntk_matrix_absscaled(NTK_component,epoch,save_str,vmin,vmax):\n",
    "    '''\n",
    "    plots a heatmap of the NTK, and does so by scaling the data first to be between -1 and 1\n",
    "    '''\n",
    "    NTK_component = NTK_component.detach().cpu().numpy()\n",
    "    #\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    plt.imshow(NTK_component,cmap='Greys',vmin=vmin, vmax=vmax)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('NTK index j')\n",
    "    plt.ylabel('NTK index i')\n",
    "    plt.title('MNIST-2 {} matrix map, step={}'.format(save_str,str(epoch).zfill(5)))\n",
    "    \n",
    "    #plt.savefig(os.path.join('./images','matrixMNIST2_abs_'+save_str+'_'+str(epoch).zfill(5)+'.pdf'))\n",
    "    #plt.close()\n",
    "    #plt.show()\n",
    "    plt.savefig(os.path.join(experiment_base_path,'images_matrixmapscaled',save_str,str(epoch).zfill(5)+'.png'))\n",
    "    plt.close()\n",
    "    \n",
    "def distributions(NTK_component,epoch,save_str):\n",
    "    '''\n",
    "    plots the distribution of all values in sixs and all values in nines\n",
    "    '''\n",
    "    NTK_component = NTK_component.detach().cpu().numpy()\n",
    "    \n",
    "    \n",
    "    #This would be used to determine if the value is six or nine, not the distribution.\n",
    "    sixes_and_sixes = NTK_component[0:5000,0:5000]\n",
    "    sixes_and_nines= NTK_component[0:5000,5000::]\n",
    "                            \n",
    "    nines_and_sixes= NTK_component[5000::,0:5000]\n",
    "    nines_and_nines = NTK_component[5000::,5000::]\n",
    "    \n",
    "    #Now  plot two things: first the distribution of 6s with 6s and the distribution of 6s with 9s\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    bins = np.linspace(np.quantile(NTK_component[0:5000,:],0.001), np.quantile(NTK_component[0:5000,:],0.999), 300)\n",
    "    plt.hist(sixes_and_sixes.flatten(),histtype='step',color='tab:blue',label='6s with 6s',bins=bins)\n",
    "    plt.hist(sixes_and_nines.flatten(),histtype='step',color='tab:orange',label='6s with 9s',bins=bins)\n",
    "    plt.title('MNIST-2 {} Histogram of 6s, step={}'.format(save_str,str(epoch).zfill(5)))\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join('./images','distributionMNIST2_'+save_str+'_'+str(epoch).zfill(5)+'.png'))\n",
    "    plt.close()\n",
    "    #plt.show()\n",
    "    \n",
    "def accuracy_kernels(model,x,y,X,Y):\n",
    "    '''\n",
    "    According to a user, matrix inversion of 10k x 10k matrices is a very costly operation.\n",
    "    \n",
    "    And this is a complex operation since it requires calculating the Jacobian of x and having the Jacobian of X\n",
    "    handy. I dont store those numbers, so if you wanted to be useful you would need to \n",
    "    '''\n",
    "    index_test = len(x)\n",
    "    __, params = forward(model,torch.cat([x,X]))\n",
    "    kernel_components = explicit_ntk(**params)\n",
    "    Y_prime = Y.clone()\n",
    "    Y_prime[Y_prime==0] = -1\n",
    "    y_prime = y.clone()\n",
    "    y_prime[y_prime==0] = -1\n",
    "    layer=1\n",
    "    for i in reversed(range(len(kernel_components))):\n",
    "        f_x = torch.sign(torch.sum(Y_prime*kernel_components[i][0:index_test,index_test::],dim=1))\n",
    "        #should be len index_test vector\n",
    "        correct = torch.sum(f_x == y_prime)\n",
    "        print('layer {} kernel acc: {:.2f}'.format(i+1,100*correct/index_test))\n",
    "    ntk = torch.sum(torch.stack(kernel_components),dim=0)\n",
    "    f_x = torch.sign(torch.sum(Y_prime*kernel_components[i][0:index_test,index_test::],dim=1))\n",
    "    correct = torch.sum(f_x == y_prime)\n",
    "    print('full ntk kernel acc: {:.2f}'.format(100*correct/index_test))\n",
    "    \n",
    "    del kernel_components[:]\n",
    "    del kernel_components\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3754215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ec10353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████████████▌                    | 75/100 [00:14<00:03,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 4 kernel acc: 89.27\n",
      "layer 3 kernel acc: 95.02\n",
      "layer 2 kernel acc: 93.39\n",
      "layer 1 kernel acc: 91.31\n",
      "full ntk kernel acc: 91.31\n",
      "14.860041856765747\n",
      "0.0013391971588134766\n",
      "0.0013425350189208984\n",
      "0.001352548599243164\n",
      "0.0013248920440673828\n",
      "0.0013248920440673828\n",
      "0.001329660415649414\n",
      "0.0013113021850585938\n",
      "0.0013134479522705078\n",
      "0.001323699951171875\n",
      "0.001316070556640625\n",
      "0.001316070556640625\n",
      "0.0013124942779541016\n",
      "0.001310110092163086\n",
      "0.0013203620910644531\n",
      "0.0013186931610107422\n",
      "0.0013124942779541016\n",
      "0.001337289810180664\n",
      "0.0013284683227539062\n",
      "0.0013120174407958984\n",
      "0.001314401626586914\n",
      "0.0013141632080078125\n",
      "0.0013194084167480469\n",
      "0.0013148784637451172\n",
      "0.001313924789428711\n",
      "0.0013229846954345703\n",
      "0.001325845718383789\n",
      "0.001310110092163086\n",
      "0.0013229846954345703\n",
      "0.0013251304626464844\n",
      "0.001317739486694336\n",
      "0.0013120174407958984\n",
      "0.0013213157653808594\n",
      "0.0013124942779541016\n",
      "0.0013134479522705078\n",
      "0.001323699951171875\n",
      "0.001312255859375\n",
      "0.0013234615325927734\n",
      "0.0013217926025390625\n",
      "0.0013146400451660156\n",
      "0.0013203620910644531\n",
      "0.0013189315795898438\n",
      "0.0013153553009033203\n",
      "0.0013091564178466797\n",
      "0.0013246536254882812\n",
      "0.0013129711151123047\n",
      "0.0013279914855957031\n",
      "0.0013167858123779297\n",
      "0.0013110637664794922\n",
      "0.0013251304626464844\n",
      "0.0013170242309570312\n",
      "0.0013165473937988281\n",
      "0.0013225078582763672\n",
      "0.0013079643249511719\n",
      "0.0013134479522705078\n",
      "0.001325368881225586\n",
      "0.0013129711151123047\n",
      "0.0013153553009033203\n",
      "0.0013270378112792969\n",
      "0.0013124942779541016\n",
      "0.0013093948364257812\n",
      "0.0013287067413330078\n",
      "0.0013189315795898438\n",
      "0.0013163089752197266\n",
      "0.0013279914855957031\n",
      "0.0013146400451660156\n",
      "0.0013203620910644531\n",
      "0.0013263225555419922\n",
      "0.0013117790222167969\n",
      "0.0013136863708496094\n",
      "0.0013248920440673828\n",
      "0.0013091564178466797\n",
      "0.0013146400451660156\n",
      "0.0013217926025390625\n",
      "0.001310586929321289\n",
      "0.0012803077697753906\n",
      "0.0013279914855957031\n",
      "0.001312255859375\n",
      "0.0013227462768554688\n",
      "0.0013141632080078125\n",
      "0.0013117790222167969\n",
      "0.0013227462768554688\n",
      "0.0013191699981689453\n",
      "0.0013146400451660156\n",
      "0.001313924789428711\n",
      "0.0013127326965332031\n",
      "0.0013065338134765625\n",
      "0.0013189315795898438\n",
      "0.0013206005096435547\n",
      "0.0013222694396972656\n",
      "0.0013277530670166016\n",
      "0.0013251304626464844\n",
      "0.0013079643249511719\n",
      "0.0013229846954345703\n",
      "0.0013194084167480469\n",
      "0.0013103485107421875\n",
      "0.00131988525390625\n",
      "0.0013132095336914062\n",
      "0.0013210773468017578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 100/100 [00:29<00:00,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 4 kernel acc: 89.32\n",
      "layer 3 kernel acc: 95.17\n",
      "layer 2 kernel acc: 93.39\n",
      "layer 1 kernel acc: 91.46\n",
      "full ntk kernel acc: 91.46\n",
      "14.679290771484375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "losses=[]\n",
    "accuracies=[]\n",
    "\n",
    "step=0\n",
    "#while True:\n",
    "for step in tqdm(range(1,N_EPOCHS+1)):\n",
    "    now = time.time()\n",
    "    \n",
    "    model.zero_grad()\n",
    "    \n",
    "    output, params = forward(model, train_x)\n",
    "    \n",
    "    if step==1 or step==N_EPOCHS:\n",
    "        components = explicit_ntk(**params)\n",
    "        \n",
    "        visualize_ntk_matrix_absscaled(components[-4],step,'layer4',-0.3367,0.4031)\n",
    "        visualize_ntk_matrix_absscaled(components[-3],step,'layer3',-0.2959,0.7079)\n",
    "        visualize_ntk_matrix_absscaled(components[-2],step,'layer2',-0.3990,1.8657)\n",
    "        visualize_ntk_matrix_absscaled(components[-1],step,'layer1',-13.1301,421.7019)\n",
    "        visualize_ntk_matrix_absscaled(torch.sum(torch.stack(components),dim=0),step,'ntk',-13.1301,421.7019)\n",
    "        \n",
    "        del components[:]\n",
    "        del components\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    if step==1 or step==N_EPOCHS:\n",
    "        accuracy_kernels(model,test_x,test_y,train_x,train_y)\n",
    "    \n",
    "    loss = criterion(output,train_y)\n",
    "    loss.backward()\n",
    "    losses.append(loss.item())\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for name,W in model.named_parameters():\n",
    "            W-= LR * W.grad\n",
    "            \n",
    "    predictions = torch.round(torch.sigmoid(output))\n",
    "    acc = torch.sum(predictions==train_y)\n",
    "    accuracies.append(acc.detach().cpu().numpy()/len(train_y))\n",
    "    print(time.time() - now)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c75c143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6663719415664673\n"
     ]
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.title('MNIST-2 Training Loss')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.xlabel('Step')\n",
    "plt.savefig(os.path.join(experiment_base_path,'MNIST2_losses.pdf'))\n",
    "plt.close()\n",
    "\n",
    "plt.plot(accuracies,label='train')\n",
    "plt.title('MNIST-2 Training Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Step')\n",
    "plt.ylim(-0.01,1.01)\n",
    "plt.savefig(os.path.join(experiment_base_path,'MNIST2_accuracies.pdf'))\n",
    "plt.close()\n",
    "\n",
    "print(losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15b5980",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(losses[0])\n",
    "print(losses[-1])\n",
    "\n",
    "print(accuracies[0])\n",
    "print(accuracies[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
