{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e8a192d",
   "metadata": {},
   "source": [
    "# Synopsis: 1) Jax still doesn't work on Windows, but 2) jax jit is faster than numba for small networks and larger number of datapoints. (haven't tested other scenarios)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa56582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch import load\n",
    "from torch.nn import functional as F\n",
    "from torch import autograd\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import time\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from numba import njit\n",
    "from jax import numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc61cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identity\n",
    "def activation(x):\n",
    "    return x\n",
    "\n",
    "#@njit\n",
    "def d_activation(x):\n",
    "    #return 1 \n",
    "    return np.ones(np.shape(x),dtype=np.float32) #this should be a differnt value...?\n",
    "    #return np.eye(np.shape(x)[0],np.shape(x)[1])\n",
    "\n",
    "#Tanh\n",
    "# def activation(x):\n",
    "#     return torch.tanh(x)\n",
    "\n",
    "#@njit\n",
    "# def d_activation(x):\n",
    "#     return np.cosh(x)**-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ea6dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_activation = njit(d_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc32732",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = 5\n",
    "hidden_layer = 128\n",
    "N_datapoints = 100\n",
    "SEED = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7404bf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cross(X):\n",
    "    return np.dot(np.transpose(X),X)\n",
    "\n",
    "\n",
    "def compute_NTK(Ws, Xs, d_int, d_array):#L counts from 1 to number of layers.\n",
    "    '''\n",
    "    I should add some docstring\n",
    "    \n",
    "    Ws, a list of the weights as np.array type np.float32,                          [W1, W2, W3 ... W]\n",
    "    Xs, a list of the conjugate kernels as np.array type np.float32,            [X0, X1, X2, ... XL]\n",
    "    d_int, a list of the dimensionality of X_l as int64,                        [d0, d1, d2, ... dL]\n",
    "    d_array, a list of the dimensionality of X_l, as np.array type np.float 32, [d0, d1, d2, ... dL] \n",
    "    all of this is neccessary because numba doesnt like type conversion.\n",
    "    \n",
    "    outputs the NTK as a np.array of type np.float32\n",
    "    '''\n",
    "    L = len(Xs)-1 #number of layers, Xs goes from inputs to right before outputs; X_0 is the input, X_L CK\n",
    "    n = Xs[0].shape[1] #number of datapoints\n",
    "    Ds = [np.array([[0.0]],dtype=np.float32)] #holds the derivatives, first value is empty list...?; just a spacer, replace with array\n",
    "    for l in range(L):\n",
    "        Ds.append(d_activation(np.dot(Ws[l],Xs[l])))\n",
    "    KNTK = cross(Xs[L]) #this is eventually summed over\n",
    "    #print(L+1,KNTK)\n",
    "    for l in range(1,L+1):\n",
    "        #we are going to construct terms that look like ( S^T S ) * (X^T X)\n",
    "        XtX = cross(Xs[l-1])\n",
    "        S = np.zeros((d_int[l],n),dtype=np.float32)\n",
    "        for i in range(n):\n",
    "            s = Ws[-1].T.reshape(-1)/np.sqrt(d_array[L])\n",
    "            for k in range(L,l-1,-1):\n",
    "                s = Ds[k][:,i]*s\n",
    "                if k > l:\n",
    "                    s = np.dot(Ws[k-1],s)/np.sqrt(d_array[k-1])\n",
    "            S[:,i] = s\n",
    "        #print(l,cross(S)*XtX)\n",
    "        KNTK += cross(S) * XtX\n",
    "    return KNTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da6dba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross = njit(cross)\n",
    "\n",
    "compute_NTK = njit(compute_NTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c5912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NTK_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        print(m.weight.shape)\n",
    "        nn.init.normal_(m.weight.data)#/m.weight.shape[0]\n",
    "        if m.bias != None:\n",
    "            nn.init.normal_(m.bias.data)#/m.weight.shape[0]\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        print(m.weight.shape)\n",
    "        nn.init.normal_(m.weight.data)#/m.weight.shape[0]\n",
    "        if m.bias != None:\n",
    "            nn.init.normal_(m.bias.data)#/m.weight.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468870e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layerwise Needs each conjugate Kernel\n",
    "class dumb_small_layerwise(torch.nn.Module):\n",
    "    '''\n",
    "    simple network for test cases\n",
    "    '''\n",
    "    def __init__(self,):\n",
    "        super(dumb_small_layerwise, self).__init__()\n",
    "        \n",
    "        self.d1 = torch.nn.Linear(5,256,bias=False)\n",
    "        self.d2 = torch.nn.Linear(256,256,bias=False)\n",
    "        self.d3 = torch.nn.Linear(256,1,bias=False)\n",
    "        \n",
    "    def forward(self, x_0):\n",
    "        x_1 = activation(self.d1(x_0)) / np.sqrt(256)\n",
    "        x_2 = activation(self.d2(x_1)) / np.sqrt(256)\n",
    "        x_3 = activation(self.d3(x_2))\n",
    "        \n",
    "        return x_3, x_2, x_1, x_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2643e215",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "device='cpu'\n",
    "\n",
    "model_small = dumb_small_layerwise()\n",
    "model_small.to(device)\n",
    "model_small.apply(NTK_weights)\n",
    "\n",
    "#Reset the seed and \n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "model_layerwise = dumb_small_layerwise()\n",
    "model_layerwise.to(device)\n",
    "model_layerwise.apply(NTK_weights)\n",
    "\n",
    "x_test = np.random.normal(0,1,(200,5)).astype(np.float32)\n",
    "x_test = torch.from_numpy(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32272caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_3, x_2, x_1, x_0 = model_layerwise(x_test)\n",
    "\n",
    "Ws = []\n",
    "Ws.append(model_layerwise.d1.weight.detach().numpy().astype(np.float32))\n",
    "Ws.append(model_layerwise.d2.weight.detach().numpy().astype(np.float32))\n",
    "Ws.append(model_layerwise.d3.weight.detach().numpy().astype(np.float32))\n",
    "\n",
    "Xs = [] # Xs are shape (output x #DP) ; however, typical python notation is reversed, so we take transpose here\n",
    "Xs.append(x_0.detach().numpy().T.astype(np.float32))\n",
    "Xs.append(x_1.detach().numpy().T.astype(np.float32))\n",
    "Xs.append(x_2.detach().numpy().T.astype(np.float32))\n",
    "\n",
    "ds_int = []\n",
    "ds_int.append(5)\n",
    "ds_int.append(256)\n",
    "ds_int.append(256)\n",
    "\n",
    "ds_array = []\n",
    "ds_array.append(np.array([5.0],dtype=np.float32)) #first element is the input length\n",
    "ds_array.append(np.array([256.0],dtype=np.float32))\n",
    "ds_array.append(np.array([256.0],dtype=np.float32)) #the remaining elements are the output lengths, but omit the last output length assumed 1.\n",
    "\n",
    "NTK_layerwise = compute_NTK(Ws, Xs, ds_int, ds_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be15a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit #288ms\n",
    "NTK_layerwise = compute_NTK(Ws, Xs, ds_int, ds_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e96232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jax_cross(X):\n",
    "    return jnp.dot(jnp.transpose(X),X)\n",
    "\n",
    "def jax_d_activation(x):\n",
    "    return jnp.array(np.ones(np.shape(x),dtype=np.float32)) #this should be a differnt value...?\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7882cf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jax_compute_NTK(Ws, Xs, d_int, d_array):#L counts from 1 to number of layers.\n",
    "    '''\n",
    "    I should add some docstring\n",
    "    \n",
    "    Ws, a list of the weights as np.array type np.float32,                          [W1, W2, W3 ... W]\n",
    "    Xs, a list of the conjugate kernels as np.array type np.float32,            [X0, X1, X2, ... XL]\n",
    "    d_int, a list of the dimensionality of X_l as int64,                        [d0, d1, d2, ... dL]\n",
    "    d_array, a list of the dimensionality of X_l, as np.array type np.float 32, [d0, d1, d2, ... dL] \n",
    "    all of this is neccessary because numba doesnt like type conversion.\n",
    "    \n",
    "    outputs the NTK as a np.array of type np.float32\n",
    "    '''\n",
    "    L = len(Xs)-1 #number of layers, Xs goes from inputs to right before outputs; X_0 is the input, X_L CK\n",
    "    n = Xs[0].shape[1] #number of datapoints\n",
    "    Ds = [jnp.array(np.array([[0.0]],dtype=np.float32))] #holds the derivatives, first value is empty list...?; just a spacer, replace with array\n",
    "    for l in range(L):\n",
    "        Ds.append(jax_d_activation(jnp.dot(Ws[l],Xs[l])))\n",
    "    KNTK = jax_cross(Xs[L]) #this is eventually summed over\n",
    "    #print(L+1,KNTK)\n",
    "    for l in range(1,L+1):\n",
    "        #we are going to construct terms that look like ( S^T S ) * (X^T X)\n",
    "        XtX = jax_cross(Xs[l-1])\n",
    "        S = jnp.array(np.zeros((d_int[l],n),dtype=np.float32)) #d_int is a 'tracer'\n",
    "        for i in range(n):\n",
    "            s = Ws[-1].T.reshape(-1)/jnp.sqrt(d_array[L])\n",
    "            for k in range(L,l-1,-1):\n",
    "                s = Ds[k][:,i]*s\n",
    "                if k > l:\n",
    "                    s = jnp.dot(Ws[k-1],s)/jnp.sqrt(d_array[k-1])\n",
    "            S.at[:,i].set(s)\n",
    "        #print(l,cross(S)*XtX)\n",
    "        KNTK += jax_cross(S) * XtX\n",
    "    return KNTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b728a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax_NTK_layerwise = jax_compute_NTK(Ws, Xs, ds_int, ds_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0b6b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit\n",
    "\n",
    "jax_compute_NTK = jit(jax_compute_NTK, static_argnums=(2,))\n",
    "jax_cross = jit(jax_cross)\n",
    "jax_d_activation = jit(jax_d_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cf5498",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_int_tuple = (5, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7464a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jaxjit_NTK_layerwise = jax_compute_NTK(Ws, Xs, ds_int_tuple, ds_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
