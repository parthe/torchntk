{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e2a2603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from easy_ntk import calculate_NTK\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# import argparse\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--ZMIN',default=0.0,type=float)\n",
    "# parser.add_argument('--ZMAX',default=1.0,type=float)\n",
    "# parser.add_argument('--BATCH_SIZE',default=64,type=int)\n",
    "# parser.add_argument('--N_EPOCHS',default=100,type=int)\n",
    "# parser.add_argument('--LR',default=1e-4,type=float)\n",
    "# parser.add_argument('--WIDTH',default=500,type=int)\n",
    "# parser.add_argument('--TRAIN_FLAG',default=False,type=bool)\n",
    "# parser.add_argument('--NTK_FLAG',default=False,type=bool)\n",
    "# parser.add_argument('--NTK_POINTS',default=1000,type=int)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# ZMIN = args.ZMIN\n",
    "# ZMAX = args.ZMAX\n",
    "# BATCH_SIZE = args.BATCH_SIZE\n",
    "# N_EPOCHS = args.N_EPOCHS\n",
    "# LR = args.LR\n",
    "# WIDTH = args.WIDTH\n",
    "# TRAIN_FLAG = args.TRAIN_FLAG\n",
    "# NTK_FLAG = args.NTK_FLAG\n",
    "# NTK_POINTS = args.NTK_POINTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffbede72",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './../DATA/no_repeats.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e374289af01c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m        'zFmeanflxR7', 'yFmeanflxR7']\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./../DATA/no_repeats.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_columns\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#!! Download this file!, see README\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ZBEST'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\pytorch1.9\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\pytorch1.9\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\pytorch1.9\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\pytorch1.9\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\pytorch1.9\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\pytorch1.9\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\pytorch1.9\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './../DATA/no_repeats.csv'"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir=None) #this defaults to saving to './runs'\n",
    "#calling this starts a new run, so dont be afraid to.\n",
    "\n",
    "ZMIN = 0.0\n",
    "ZMAX = 1.0\n",
    "BATCH_SIZE = 256\n",
    "N_EPOCHS = 10\n",
    "LR = 1e-5\n",
    "WIDTH = 500\n",
    "TRAIN_FLAG = True\n",
    "NTK_FLAG = True\n",
    "NTK_POINTS = 500\n",
    "DEVICE='cpu'\n",
    "###########################################\n",
    "use_columns = ['ZBEST','gFKronFlux', 'rFKronFlux', 'iFKronFlux', 'zFKronFlux',\n",
    "       'yFKronFlux', 'gFPSFFlux', 'rFPSFFlux', 'iFPSFFlux',\n",
    "       'zFPSFFlux', 'yFPSFFlux', 'gFApFlux', 'rFApFlux', 'iFApFlux',\n",
    "       'zFApFlux', 'yFApFlux','gFmeanflxR5', 'rFmeanflxR5', 'iFmeanflxR5',\n",
    "       'zFmeanflxR5', 'yFmeanflxR5','gFmeanflxR6',\n",
    "       'rFmeanflxR6', 'iFmeanflxR6', 'zFmeanflxR6', 'yFmeanflxR6',\n",
    "       'gFmeanflxR7', 'rFmeanflxR7', 'iFmeanflxR7',\n",
    "       'zFmeanflxR7', 'yFmeanflxR7','raMean','decMean']\n",
    "\n",
    "X_COLUMNS = ['gFKronFlux', 'rFKronFlux', 'iFKronFlux', 'zFKronFlux',\n",
    "       'yFKronFlux', 'gFPSFFlux', 'rFPSFFlux', 'iFPSFFlux',\n",
    "       'zFPSFFlux', 'yFPSFFlux', 'gFApFlux', 'rFApFlux', 'iFApFlux',\n",
    "       'zFApFlux', 'yFApFlux','gFmeanflxR5', 'rFmeanflxR5', 'iFmeanflxR5',\n",
    "       'zFmeanflxR5', 'yFmeanflxR5','gFmeanflxR6',\n",
    "       'rFmeanflxR6', 'iFmeanflxR6', 'zFmeanflxR6', 'yFmeanflxR6',\n",
    "       'gFmeanflxR7', 'rFmeanflxR7', 'iFmeanflxR7',\n",
    "       'zFmeanflxR7', 'yFmeanflxR7']\n",
    " \n",
    "DF = pd.read_csv('./../DATA/no_repeats.csv',usecols=use_columns) #!! Download this file!, see README\n",
    "\n",
    "Y = DF['ZBEST'].values\n",
    "X = DF[X_COLUMNS].values\n",
    "\n",
    "#Pair down to Y < 1\n",
    "X = X[Y<ZMAX]\n",
    "Y = Y[Y<ZMAX]\n",
    "\n",
    "X = X[Y>=ZMIN]\n",
    "Y = Y[Y>=ZMIN]\n",
    "\n",
    "#m = -2.5/ln(10) * [asinh((f/f0)/(2b)) + ln(b)]\n",
    "\n",
    "#The asinh magnitudes are characterized by a softening parameter b, the typical 1-sigma \n",
    "#noise of the sky in a PSF aperture in 1'' seeing. The relation between detected flux f and asinh magnitude m is:\n",
    "\n",
    "f_0 = 3631 #Jy\n",
    "\n",
    "#https://iopscience.iop.org/article/10.1088/0004-637X/756/2/158/pdf table1\n",
    "\n",
    "#1 square arcsecond sky background magnitude, use it to find b\n",
    "g_mu = 21.92\n",
    "r_mu = 20.83\n",
    "i_mu = 19.79\n",
    "z_mu = 19.24\n",
    "y_mu = 18.24\n",
    "\n",
    "b_g = np.exp((g_mu*np.log(10)/-2.5) - np.arcsinh((1/(2*f_0))))\n",
    "b_r = np.exp((r_mu*np.log(10)/-2.5) - np.arcsinh((1/(2*f_0))))\n",
    "b_i = np.exp((i_mu*np.log(10)/-2.5) - np.arcsinh((1/(2*f_0))))\n",
    "b_z = np.exp((z_mu*np.log(10)/-2.5) - np.arcsinh((1/(2*f_0))))\n",
    "b_y = np.exp((y_mu*np.log(10)/-2.5) - np.arcsinh((1/(2*f_0))))\n",
    "\n",
    "def convert_flux_to_luptitude(f,b,f_0=3631):\n",
    "    return -2.5/np.log(10) * (np.arcsinh((f/f_0)/(2*b)) + np.log(b))\n",
    "\n",
    "#g\n",
    "X[:,[0,5,10,15,20,25]] = convert_flux_to_luptitude(X[:,[0,5,10,15,20,25]],b=b_g)\n",
    "\n",
    "#r\n",
    "X[:,[1,6,11,16,21,26]] = convert_flux_to_luptitude(X[:,[1,6,11,16,21,26]],b=b_r)\n",
    "\n",
    "#i\n",
    "X[:,[2,7,12,17,22,27]] = convert_flux_to_luptitude(X[:,[2,7,12,17,22,27]],b=b_i)\n",
    "\n",
    "#z\n",
    "X[:,[3,8,13,18,23,28]] = convert_flux_to_luptitude(X[:,[3,8,13,18,23,28]],b=b_z)\n",
    "\n",
    "#y\n",
    "X[:,[4,9,14,19,24,29]] = convert_flux_to_luptitude(X[:,[4,9,14,19,24,29]],b=b_y)\n",
    "###########################################\n",
    "#Robust to outliers\n",
    "\n",
    "MEANS = np.median(X,axis=0)\n",
    "IQR = np.quantile(X,axis=0,q=[0.75,0.25])\n",
    "STDS = (IQR[0,] - IQR[1,]) / 1.34896\n",
    "\n",
    "\n",
    "X = (X - MEANS)/STDS\n",
    "\n",
    "#robust to missing data\n",
    "X[np.isnan(X)] = -20\n",
    "#Fix outliers\n",
    "X[X<-20] = -20\n",
    "X[X>20] = 20\n",
    "\n",
    "print(np.shape(X))\n",
    "\n",
    "indices = np.arange(len(X))\n",
    "SEED=0\n",
    "random.seed(SEED)\n",
    "\n",
    "random.shuffle(indices)\n",
    "\n",
    "train_indices = indices[int(0*len(indices)):int(0.8*len(indices))]\n",
    "test_indices = indices[int(0.8*len(indices)):int(0.9*len(indices))]\n",
    "val_indices = indices[int(0.9*len(indices)):int(1.0*len(indices))]\n",
    "\n",
    "X = X.astype(np.float32)\n",
    "Y = Y.astype(np.float32)\n",
    "\n",
    "X_train = X[train_indices]\n",
    "Y_train = Y[train_indices]\n",
    "\n",
    "X_test = X[test_indices]\n",
    "Y_test = Y[test_indices]\n",
    "\n",
    "X_val = X[val_indices]\n",
    "Y_val = Y[val_indices]\n",
    "###########################################\n",
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "X_val_tensor = torch.from_numpy(X_val)\n",
    "\n",
    "Y_train_tensor = torch.from_numpy(Y_train)\n",
    "Y_test_tensor = torch.from_numpy(Y_test)\n",
    "Y_val_tensor = torch.from_numpy(Y_val)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor,Y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor,Y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor,Y_test_tensor)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset,batch_size=BATCH_SIZE,shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=BATCH_SIZE,shuffle=False)\n",
    "###########################################\n",
    "#ARCHITECTURE:\n",
    "def relu(X,normalize=True):\n",
    "    X = F.relu(X)\n",
    "    if normalize:\n",
    "        return np.sqrt(2*np.pi/(np.pi-1))*(X-1/np.sqrt(2*np.pi))\n",
    "    else:\n",
    "        return X\n",
    "\n",
    "N_features = np.shape(X_test)[1]\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Linear(N_features,WIDTH,bias=True)\n",
    "        self.l2 = nn.Linear(WIDTH,1,bias=True)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = relu(self.l1(x))\n",
    "        x = self.l2(x)\n",
    "        return x\n",
    "\n",
    "def init_weights(m): #this is lecunn initialization?\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight.data) / np.sqrt(m.weight.shape[1])\n",
    "        if isinstance(m.bias,torch.Tensor):\n",
    "            nn.init.normal_(m.bias.data) \n",
    "    \n",
    "model = MLP()\n",
    "model.to(DEVICE)\n",
    "model.apply(init_weights)\n",
    "optim = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "N_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "###########################################\n",
    "#TRAINING/EVAL LOOPS:\n",
    "loss = torch.nn.MSELoss()\n",
    "def train(epoch):\n",
    "    losses=[]\n",
    "    for idx,(x,label) in enumerate(train_dataloader):\n",
    "        x = x.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        y = model(x).view(-1)\n",
    "        output = loss(label,y)\n",
    "        losses.append(output.item())\n",
    "        output.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "    return np.mean(losses)\n",
    "        \n",
    "    \n",
    "def val(epoch):\n",
    "    model.eval()\n",
    "    losses=[]\n",
    "    with torch.no_grad():\n",
    "        for idx,(x,label) in enumerate(val_dataloader):\n",
    "            x = x.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            y = model(x).view(-1)\n",
    "            output = loss(label,y)\n",
    "            losses.append(output.item())\n",
    "    return np.mean(losses)\n",
    "        \n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    losses=[]\n",
    "    all_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for idx,(x,label) in enumerate(test_dataloader):\n",
    "            x = x.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            y = model(x).view(-1)\n",
    "            output = loss(label,y)\n",
    "            losses.append(output.item())\n",
    "            all_outputs.append(y.detatch().to('cpu').numpy())\n",
    "    return np.mean(losses), np.array(all_outputs)\n",
    "##################################################\n",
    "\n",
    "if NTK_FLAG:\n",
    "    NTK_start=time.time()\n",
    "    NTK = calculate_NTK(model,X_train_tensor[0:NTK_POINTS])\n",
    "    NTK_end = time.time()\n",
    "    print('NTK time: {:4e}s'.format(NTK_end-NTK_start),flush=True)\n",
    "    NTK_time = NTK_end-NTK_start\n",
    "    eigenvalues, eigenvectors = torch.torch.linalg.eigh(NTK)\n",
    "    writer.add_histogram('NTK',torch.log(eigenvalues)/(np.log(10)),0,bins='auto')\n",
    "    writer.add_scalar('NTK/Condition_number',torch.min(eigenvalues)/torch.max(eigenvalues),0)\n",
    "    writer.add_scalar('NTK/min_eigenvalue',torch.min(eigenvalues),0)\n",
    "    writer.add_scalar('NTK/max_eigenvalue',torch.max(eigenvalues),0)\n",
    "\n",
    "\n",
    "##################################################   \n",
    "all_train_loss=[]\n",
    "all_val_loss=[]\n",
    "if TRAIN_FLAG:\n",
    "    for epoch in range(1,N_EPOCHS+1):\n",
    "        epoch_start=time.time()\n",
    "        train_loss=train(epoch)\n",
    "        all_train_loss.append(train_loss)\n",
    "        train_end=time.time()\n",
    "        val_loss=val(epoch)\n",
    "        all_val_loss.append(val_loss)\n",
    "        val_end=time.time()\n",
    "        \n",
    "        writer.add_scalar('Loss/train',train_loss,epoch)\n",
    "        writer.add_scalar('Loss/validation',val_loss,epoch)\n",
    "        \n",
    "        print('train loss: {:4e} val loss: {:4e} train time: {:4e}s'.format(train_loss,val_loss,train_end-epoch_start))\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                writer.add_histogram(name,param.data,epoch,bins='auto')\n",
    "                \n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and 'weight' in name:\n",
    "                writer.add_histogram('X_'+name,torch.linalg.eigh(torch.matmul(param.data,param.data.T))[0],epoch,bins='auto')\n",
    "                \n",
    "        NTK_start=time.time()\n",
    "        NTK = calculate_NTK(model,X_train_tensor[0:NTK_POINTS])\n",
    "        NTK_end = time.time()\n",
    "        NTK_time = NTK_end-NTK_start\n",
    "        eigenvalues, eigenvectors = torch.torch.linalg.eigh(NTK)\n",
    "        writer.add_histogram('NTK',torch.log(eigenvalues)/(np.log(10)),epoch,bins='auto')\n",
    "        writer.add_scalar('NTK/Condition_number',torch.min(eigenvalues)/torch.max(eigenvalues),epoch)\n",
    "        writer.add_scalar('NTK/min_eigenvalue',torch.min(eigenvalues),epoch)\n",
    "        writer.add_scalar('NTK/max_eigenvalue',torch.max(eigenvalues),epoch)\n",
    "        #print('NTK time: {:4e}s'.format(NTK_end-NTK_start),flush=True)\n",
    "#########################################\n",
    "writer.add_hparams({'ZMIN':ZMIN,\n",
    "                      'ZMAX':ZMAX,\n",
    "                      'BATCH_SIZE':BATCH_SIZE,\n",
    "                      'N_EPOCHS ':N_EPOCHS,\n",
    "                      'LR':LR,\n",
    "                      'WIDTH':WIDTH,\n",
    "                      'NTK_POINTS':NTK_POINTS,\n",
    "                      'NUM_PARAMS':N_parameters},{})\n",
    "\n",
    "writer.flush() #writes everything to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e66564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    losses=[]\n",
    "    all_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for idx,(x,label) in enumerate(test_dataloader):\n",
    "            x = x.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            y = model(x).view(-1)\n",
    "            output = loss(label,y)\n",
    "            losses.append(output.item())\n",
    "            all_outputs.extend(y.detach().to('cpu').numpy())\n",
    "    return np.mean(losses), np.array(all_outputs)\n",
    "\n",
    "test_loss, out_y_test = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997c9b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Y_test,out_y_test,'.')\n",
    "plt.ylabel('Z real')\n",
    "plt.xlabel('Z est ')\n",
    "plt.title('Why this so bad?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51fe61f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
