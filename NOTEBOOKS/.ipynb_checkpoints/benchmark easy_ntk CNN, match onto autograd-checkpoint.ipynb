{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46853abf",
   "metadata": {},
   "source": [
    "# This notebook first demonstrates the basic principle behind our method through autograd, then uses analytic derivatives to find the ntk in parallel\n",
    "\n",
    "# the point is that for simple architectures the autograd NTK agrees with the analytic method up to a rtol typically of 1e-2 , visually inspecting shows that elements are indeed very close.\n",
    "\n",
    "# We can also use this notebook to benchmark the CNN ntk-- though note the expected time for autograd NTK to finish for large network or networks with many points is also large, consider skipping these cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52254cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from layerwise_ntk import compute_NTK_CNN\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch import load\n",
    "from torch.nn import functional as F\n",
    "from torch import autograd\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import time\n",
    "\n",
    "from numba import njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60333c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from easy_ntk import compute_NTK_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b5fd7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "how_many = 10\n",
    "width = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "656448cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    return torch.tanh(x)\n",
    "\n",
    "def d_activationt(x):\n",
    "    return torch.cosh(x)**-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7ccfcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NTK_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        print(m.weight.shape)\n",
    "        nn.init.normal_(m.weight.data)#/m.weight.shape[0]\n",
    "        if m.bias != None:\n",
    "            nn.init.normal_(m.bias.data)#/m.weight.shape[0]\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        print(m.weight.shape)\n",
    "        nn.init.normal_(m.weight.data)#/m.weight.shape[0]\n",
    "        if m.bias != None:\n",
    "            nn.init.normal_(m.bias.data)#/m.weight.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "864321d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dumb_small(torch.nn.Module):\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    def __init__(self,):\n",
    "        super(dumb_small, self).__init__()\n",
    "        \n",
    "        self.d1 = torch.nn.Conv2d(1,width,3,stride=1,padding=1,bias=True) #28 -> 28\n",
    "\n",
    "        self.d2 = torch.nn.Conv2d(width,width,3,stride=1,padding=1,bias=True) #28 -> 28\n",
    "        \n",
    "        self.d3 = torch.nn.Conv2d(width,width,3,stride=1,padding=1,bias=True) #28 -> 28\n",
    "        \n",
    "        self.d4 = torch.nn.Conv2d(width,width,3,stride=1,padding=1,bias=True) #28 -> 28\n",
    "        \n",
    "        self.d5 = torch.nn.Linear(width*28*28,1,bias=True)\n",
    "        \n",
    "    def forward(self, x_0):\n",
    "        x_1 = activation(self.d1(x_0))\n",
    "        x_2 = activation(self.d2(x_1))\n",
    "        x_3 = activation(self.d3(x_2))\n",
    "        x_4 = activation(self.d4(x_3))\n",
    "        x_5 = x_4.reshape(how_many,-1)\n",
    "        x_6 = self.d5(x_5)\n",
    "        return x_6 \n",
    "\n",
    "class dumb_small_layerwise(torch.nn.Module):\n",
    "    '''\n",
    "    NOTE: no activation function on the final layer!\n",
    "    '''\n",
    "    def __init__(self,):\n",
    "        super(dumb_small_layerwise, self).__init__()\n",
    "        \n",
    "        self.d1 = torch.nn.Conv2d(1,width,3,stride=1,padding=1,bias=True) #28 -> 28\n",
    "\n",
    "        self.d2 = torch.nn.Conv2d(width,width,3,stride=1,padding=1,bias=True) #28 -> 28\n",
    "        \n",
    "        self.d3 = torch.nn.Conv2d(width,width,3,stride=1,padding=1,bias=True) #28 -> 28\n",
    "        \n",
    "        self.d4 = torch.nn.Conv2d(width,width,3,stride=1,padding=1,bias=True) #28 -> 28\n",
    "        \n",
    "        self.d5 = torch.nn.Linear(width*28*28,1,bias=True)\n",
    "        \n",
    "    def forward(self, x_0):\n",
    "        x_1 = activation(self.d1(x_0))\n",
    "        x_2 = activation(self.d2(x_1))\n",
    "        x_3 = activation(self.d3(x_2))\n",
    "        x_4 = activation(self.d4(x_3))\n",
    "        x_5 = x_4.reshape(how_many,-1)\n",
    "        x_6 = self.d5(x_5)\n",
    "        return x_6, x_5, x_4, x_3, x_2, x_1, x_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "481ea149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 3, 3])\n",
      "torch.Size([8, 8, 3, 3])\n",
      "torch.Size([8, 8, 3, 3])\n",
      "torch.Size([8, 8, 3, 3])\n",
      "torch.Size([1, 6272])\n",
      "torch.Size([8, 1, 3, 3])\n",
      "torch.Size([8, 8, 3, 3])\n",
      "torch.Size([8, 8, 3, 3])\n",
      "torch.Size([8, 8, 3, 3])\n",
      "torch.Size([1, 6272])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "device='cpu'\n",
    "\n",
    "model = dumb_small_layerwise()\n",
    "model.apply(NTK_weights)\n",
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "model_2 = dumb_small()\n",
    "model_2.apply(NTK_weights)\n",
    "\n",
    "x_test = np.random.normal(0,1,(how_many,1,28,28)).astype(np.float32) #n c_in, h, w\n",
    "x_test = torch.from_numpy(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4709d6",
   "metadata": {},
   "source": [
    "# Autograd NTK-- uncomment and run if number used and number of fitlers are small, like datapoints under 100 and filters under 8, in order to test that the result and the result of the easy_ntk layerwise algorithm agree with oneanother. \n",
    "\n",
    "# we use this method because it most clearly exposes the NTK calculation to the reader. I'm litterally asking for pytorch to calculate the first derivatives of each output of the network with respect to the network's parameters, placing them into an array, and computing the grammian. The only difference is that I am calculating it layerwise, and adding the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7549959",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_2.zero_grad()\n",
    "#y = model_2(x_test)\n",
    "\n",
    "#in the future we would iterate over layers instead of like this...\n",
    "# layer_components_w1 = [] \n",
    "# layer_components_w2 = []\n",
    "# layer_components_w3 = []\n",
    "# layer_components_w4 = []\n",
    "# layer_components_w5 = []\n",
    "\n",
    "# layer_components_b1 = []\n",
    "# layer_components_b2 = []\n",
    "# layer_components_b3 = []\n",
    "# layer_components_b4 = []\n",
    "# layer_components_b5 = []\n",
    "\n",
    "# for i in range(len(y)):\n",
    "#     model_2.zero_grad()\n",
    "#     y[i].backward(retain_graph=True)\n",
    "#     #Get the tensors\n",
    "#     w1_grad = model_2.d1.weight.grad.detach().numpy()\n",
    "#     w2_grad = model_2.d2.weight.grad.detach().numpy()\n",
    "#     w3_grad = model_2.d3.weight.grad.detach().numpy()\n",
    "#     w4_grad = model_2.d4.weight.grad.detach().numpy()\n",
    "#     w5_grad = model_2.d5.weight.grad.detach().numpy()\n",
    "    \n",
    "#     b1_grad = model_2.d1.bias.grad.detach().numpy()\n",
    "#     b2_grad = model_2.d2.bias.grad.detach().numpy()\n",
    "#     b3_grad = model_2.d3.bias.grad.detach().numpy()\n",
    "#     b4_grad = model_2.d4.bias.grad.detach().numpy()\n",
    "#     b5_grad = model_2.d5.bias.grad.detach().numpy()\n",
    "\n",
    "#     #reshape and append. deep copy neccessary or else they are the same objects\n",
    "#     layer_components_w1.append(w1_grad.reshape(-1).copy())\n",
    "#     layer_components_w2.append(w2_grad.reshape(-1).copy())\n",
    "#     layer_components_w3.append(w3_grad.reshape(-1).copy())\n",
    "#     layer_components_w4.append(w4_grad.reshape(-1).copy())\n",
    "#     layer_components_w5.append(w5_grad.reshape(-1).copy())\n",
    "    \n",
    "#     layer_components_b1.append(b1_grad.reshape(-1).copy())\n",
    "#     layer_components_b2.append(b2_grad.reshape(-1).copy())\n",
    "#     layer_components_b3.append(b3_grad.reshape(-1).copy())\n",
    "#     layer_components_b4.append(b4_grad.reshape(-1).copy())\n",
    "#     layer_components_b5.append(b5_grad.reshape(-1).copy())\n",
    "\n",
    "# layer_components_w1 = np.array(layer_components_w1)\n",
    "# layer_components_w2 = np.array(layer_components_w2)\n",
    "# layer_components_w3 = np.array(layer_components_w3)\n",
    "# layer_components_w4 = np.array(layer_components_w4)\n",
    "# layer_components_w5 = np.array(layer_components_w5)\n",
    "\n",
    "# layer_components_b1 = np.array(layer_components_b1)\n",
    "# layer_components_b2 = np.array(layer_components_b2)\n",
    "# layer_components_b3 = np.array(layer_components_b3)\n",
    "# layer_components_b4 = np.array(layer_components_b4)\n",
    "# layer_components_b5 = np.array(layer_components_b5)\n",
    "\n",
    "# autograd_NTK = layer_components_w1 @ layer_components_w1.T+\\\n",
    "#     layer_components_w2 @ layer_components_w2.T+\\\n",
    "#     layer_components_w3 @ layer_components_w3.T+\\\n",
    "#     layer_components_w4 @ layer_components_w4.T+\\\n",
    "#     layer_components_w5 @ layer_components_w5.T+\\\n",
    "#     layer_components_b1 @ layer_components_b1.T+\\\n",
    "#     layer_components_b2 @ layer_components_b2.T+\\\n",
    "#     layer_components_b3 @ layer_components_b3.T+\\\n",
    "#     layer_components_b4 @ layer_components_b4.T+\\\n",
    "#     layer_components_b5 @ layer_components_b5.T\n",
    "\n",
    "#autograd_NTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7524e6b",
   "metadata": {},
   "source": [
    "# Now Layerwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1598049",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test.to('cpu')\n",
    "x_6, x_5, x_4, x_3, x_2, x_1, x_0 = model(x_test)\n",
    "\n",
    "#These need to be numpy\n",
    "Ws = []\n",
    "Ws.append(torch.tensor([0.0],dtype=torch.float32)) \n",
    "Ws.append(torch.tensor([0.0],dtype=torch.float32)) \n",
    "Ws.append(torch.tensor([0.0],dtype=torch.float32))\n",
    "Ws.append(torch.tensor([0.0],dtype=torch.float32)) #spacer\n",
    "Ws.append(torch.tensor([0.0],dtype=torch.float32))\n",
    "Ws.append(model.d5.weight.detach())\n",
    "\n",
    "#Kernel Matrices, Need to be numpy\n",
    "Ks = []\n",
    "Ks.append(model.d1.weight.detach())\n",
    "Ks.append(model.d2.weight.detach())\n",
    "Ks.append(model.d3.weight.detach())\n",
    "Ks.append(model.d4.weight.detach())\n",
    "Ks.append(torch.tensor([0.0],dtype=torch.float32)) #spacer\n",
    "Ks.append(torch.tensor([0.0],dtype=torch.float32))\n",
    "\n",
    "\n",
    "Xs = [] # Xs are shape (output x #DP) ; however, typical python notation is reversed, so we take transpose here\n",
    "Xs.append(x_0.T.detach())\n",
    "Xs.append(x_1.T.detach())\n",
    "Xs.append(x_2.T.detach())\n",
    "Xs.append(x_3.T.detach())\n",
    "Xs.append(x_4.T.detach())\n",
    "Xs.append(x_5.T.detach())\n",
    "\n",
    "#This is used to create arrays-- needs to be integer list to play nice with compilers\n",
    "ds_int = []\n",
    "ds_int.append(width*3*3) #channels_out * kernel_height * kernel_width\n",
    "ds_int.append(width*3*3) #channels_out * kernel_height * kernel_width\n",
    "ds_int.append(width*3*3) #channels_out * kernel_height * kernel_width\n",
    "ds_int.append(width*3*3) #channels_out * kernel_height * kernel_width\n",
    "ds_int.append(1) #channels_out * kernel_height * kernel_width\n",
    "ds_int.append(1) #channels_out * kernel_height * kernel_width\n",
    "\n",
    "ds_array = [] #this is for the NTK formulation, \n",
    "#ds_array.append(torch.tensor([1.0],dtype=torch.float32).to(device)) #first element is a spacer, could be anything.\n",
    "\n",
    "ds_array.append(torch.tensor([1.0],dtype=torch.float32).to(device)) #first element is a spacer, could be anything.\n",
    "ds_array.append(torch.tensor([1.0],dtype=torch.float32).to(device)) #The rest, even if you dont use NTK formulation, would be 1\n",
    "ds_array.append(torch.tensor([1.0],dtype=torch.float32).to(device))\n",
    "ds_array.append(torch.tensor([1.0],dtype=torch.float32).to(device))\n",
    "ds_array.append(torch.tensor([1.0],dtype=torch.float32).to(device))\n",
    "ds_array.append(torch.tensor([1.0],dtype=torch.float32).to(device))\n",
    "\n",
    "filters = []\n",
    "filters.append(1)\n",
    "filters.append(1)\n",
    "filters.append(1)\n",
    "filters.append(1)\n",
    "filters.append(0)\n",
    "filters.append(0)\n",
    "\n",
    "\n",
    "padding = []\n",
    "padding.append(1)\n",
    "padding.append(1)\n",
    "padding.append(1)\n",
    "padding.append(1)\n",
    "padding.append(0)\n",
    "padding.append(0)\n",
    "\n",
    "\n",
    "strides = []\n",
    "strides.append(1)\n",
    "strides.append(1)\n",
    "strides.append(1)\n",
    "strides.append(1)\n",
    "strides.append(0)\n",
    "strides.append(0)\n",
    "\n",
    "\n",
    "layers=[model.d1,\n",
    "        model.d2,\n",
    "        model.d3,\n",
    "        model.d4,\n",
    "        0.0,\n",
    "        model.d5\n",
    "       ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0431f0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6703555583953857\n"
     ]
    }
   ],
   "source": [
    "now = time.time()\n",
    "ntk_components = compute_NTK_CNN(Ws, Ks, Xs, ds_int, ds_array, strides, padding, layers, d_activationt, device=\"cpu\")\n",
    "print(time.time() - now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c65aed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.3 s ± 2.61 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "ntk_components = compute_NTK_CNN(Ws, Ks, Xs, ds_int, ds_array, strides, padding, layers, d_activationt, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1212275",
   "metadata": {},
   "outputs": [],
   "source": [
    "NTK = torch.sum(torch.stack(ntk_components),[0,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490c0cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "NTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59f1b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "autograd_NTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7876b33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(NTK.cpu().numpy(),autograd_NTK,1e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
