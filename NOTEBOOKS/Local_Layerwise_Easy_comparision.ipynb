{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aceb24f1",
   "metadata": {},
   "source": [
    "# This code demonstrates that the layerwise computation is accurate in the same way that the easy ntk experiment is. We also verify that the easy_NTK gives the same answer as layerwise_NTK, lending credibility to both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "318e76b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch import load\n",
    "from torch.nn import functional as F\n",
    "from torch import autograd\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#from ..easy_ntk import calculate_NTK\n",
    "#from einops import rearrange\n",
    "\n",
    "import time\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from numba import njit\n",
    "from numba.typed import List\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext line_profiler\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c3a07aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def _del_nested_attr(obj, names):\n",
    "    \"\"\"\n",
    "    Deletes the attribute specified by the given list of names.\n",
    "    For example, to delete the attribute obj.conv.weight,\n",
    "    use _del_nested_attr(obj, ['conv', 'weight'])\n",
    "    \"\"\"\n",
    "    if len(names) == 1:\n",
    "        delattr(obj, names[0])\n",
    "    else:\n",
    "        _del_nested_attr(getattr(obj, names[0]), names[1:])\n",
    "\n",
    "def _set_nested_attr(obj, names, value):\n",
    "    \"\"\"\n",
    "    Set the attribute specified by the given list of names to value.\n",
    "    For example, to set the attribute obj.conv.weight,\n",
    "    use _del_nested_attr(obj, ['conv', 'weight'], value)\n",
    "    \"\"\"\n",
    "    if len(names) == 1:\n",
    "        setattr(obj, names[0], value)\n",
    "    else:\n",
    "        _set_nested_attr(getattr(obj, names[0]), names[1:], value)\n",
    "\n",
    "def extract_weights(mod):\n",
    "    \"\"\"\n",
    "    This function removes all the Parameters from the model and\n",
    "    return them as a tuple as well as their original attribute names.\n",
    "    The weights must be re-loaded with `load_weights` before the model\n",
    "    can be used again.\n",
    "    Note that this function modifies the model in place and after this\n",
    "    call, mod.parameters() will be empty.\n",
    "    \"\"\"\n",
    "    orig_params = tuple(mod.parameters())\n",
    "    # Remove all the parameters in the model\n",
    "    names = []\n",
    "    for name, p in list(mod.named_parameters()):\n",
    "        _del_nested_attr(mod, name.split(\".\"))\n",
    "        names.append(name)\n",
    "\n",
    "    # Make params regular Tensors instead of nn.Parameter\n",
    "    params = tuple(p.detach().requires_grad_() for p in orig_params)\n",
    "    return params, names\n",
    "\n",
    "def load_weights(mod, names, params):\n",
    "    \"\"\"\n",
    "    Reload a set of weights so that `mod` can be used again to perform a forward pass.\n",
    "    Note that the `params` are regular Tensors (that can have history) and so are left\n",
    "    as Tensors. This means that mod.parameters() will still be empty after this call.\n",
    "    \"\"\"\n",
    "    for name, p in zip(names, params):\n",
    "        _set_nested_attr(mod, name.split(\".\"), p)\n",
    "        \n",
    "def calculate_NTK(model,x,device='cpu',MODE='samples'):\n",
    "    \"\"\"\n",
    "    INPUTS:\n",
    "        model: torch.nn.Module \n",
    "        x: torch.Tensor\n",
    "        device: 'cpu',\n",
    "        MODE: 'minima'\n",
    "    \n",
    "    OUTPUTS:\n",
    "        NTK: torch.Tensor\n",
    "    \n",
    "    Calculates the NTK for a model, p_dict a state dictionary, and x, a single tensor fed into the model\n",
    "    \n",
    "    The NTK is the grammian of the Jacobian of the model output to w.r.t. the weights of the model\n",
    "    \n",
    "    This function will output the NTK such that the minima matrix size is used. If the Jacobian is an NxM\n",
    "    matrix, then the NTK is formulated so that if N < M; NTK is NxN. If M<N, then NTK is MxM.\n",
    "    \n",
    "    #EXAMPLE USAGE:\n",
    "    device='cpu'\n",
    "    model = MODEL() #a torch.nn.Module object \n",
    "    model.to(device)\n",
    "    state_dict = model.state_dict()\n",
    "\n",
    "    x_test = np.ones((100,1,28,28),dtype=np.float32)\n",
    "    x_test = torch.from_numpy(x_test)\n",
    "\n",
    "    NTK = calculate_NTK(model,x_test)\n",
    "    \"\"\"\n",
    "    if not(MODE in ['minima','samples','params']):\n",
    "        raise ValueError(\"MODE must be one of 'minima','samples','params'\")\n",
    "    \n",
    "    x = x.to(device)\n",
    "    x.requires_grad=False\n",
    "    N = x.shape[0]\n",
    "    M = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    #We need to create a clone of the model or else we make it unusable as part of the trickery \n",
    "    #to get pytorch to do what we want. Unforutantely, this exlcludes super big models. but, eh.\n",
    "    model_clone = copy.deepcopy(model)\n",
    "    \n",
    "    params, names = extract_weights(model_clone)\n",
    "    def model_ntk(*args,model=model_clone, names=names):\n",
    "        params = tuple(args)\n",
    "        load_weights(model, names, params)\n",
    "        return model(x)\n",
    "    \n",
    "    Js = torch.autograd.functional.jacobian(model_ntk, tuple(params), create_graph=False, vectorize=True)\n",
    "    \n",
    "    Js = list(Js)\n",
    "    #Js = [element for tupl in Js for element in tupl]\n",
    "    #collapse the tensors\n",
    "    for i,tensor in enumerate(Js):\n",
    "        Js[i] = tensor.reshape(N,-1)\n",
    "    \n",
    "    J = torch.cat(Js,axis=1)\n",
    "    \n",
    "    if MODE=='minima':\n",
    "        if N < M: #if datasize points is less than number of parameters:\n",
    "            NTK = torch.matmul(J,J.T)\n",
    "\n",
    "        if N >= M:#if number of parameters is less than datasize:\n",
    "            NTK = torch.matmul(J.T,J)\n",
    "    elif MODE=='samples':\n",
    "        NTK = torch.matmul(J,J.T)\n",
    "    elif MODE=='params':\n",
    "        NTK = torch.matmul(J.T,J)\n",
    "    \n",
    "    return NTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acdbfa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X,normalize=False):\n",
    "    X = F.relu(X)\n",
    "    if normalize:\n",
    "        return np.sqrt(2*np.pi/(np.pi-1))*(X-1/np.sqrt(2*np.pi))\n",
    "    else:\n",
    "        return X\n",
    "    \n",
    "\n",
    "# #Identity\n",
    "def activation(x):\n",
    "    return x\n",
    "\n",
    "# @njit\n",
    "def d_activation(x):\n",
    "    return np.ones(np.shape(x),dtype=np.float32) \n",
    "\n",
    "\n",
    "#Tanh\n",
    "#def activation(x):\n",
    "#    return torch.tanh(x)\n",
    "\n",
    "#@njit\n",
    "#def d_activation(x):\n",
    "#    return np.cosh(x)**-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5bbdd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5237787",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit #no parallel transformation available ;#fasterer\n",
    "def cross(X):\n",
    "    return X.T.dot(X)\n",
    "\n",
    "def compute_NTK(Ws, Xs, d_int, d_array):#L counts from 1 to number of layers.\n",
    "    '''\n",
    "    I should add some docstring\n",
    "    \n",
    "    Ws, a list of the weights as np.array type np.float32,                          [W1, W2, W3 ... W]\n",
    "    Xs, a list of the conjugate kernels as np.array type np.float32,            [X0, X1, X2, ... XL]\n",
    "    d_int, a list of the dimensionality of X_l as int64,                        [d0, d1, d2, ... dL]\n",
    "    d_array, a list of the dimensionality of X_l, as np.array type np.float 32, [d0, d1, d2, ... dL] \n",
    "    all of this is neccessary because numba doesnt like type conversion.\n",
    "    \n",
    "    outputs the NTK as a np.array of type np.float32\n",
    "    '''\n",
    "    L = len(Xs)-1 #number of layers, Xs goes from inputs to right before outputs; X_0 is the input, X_L CK\n",
    "    \n",
    "    n = Xs[0].shape[1] #number of datapoints\n",
    "    \n",
    "    #holds the derivatives, first value is empty list...?; just a spacer, replace with array\n",
    "    Ds = [np.array([[0.0]],dtype=np.float32)] \n",
    "    for l in range(L):\n",
    "        Ds.append(d_activation(np.dot(Ws[l],Xs[l])))\n",
    "    \n",
    "    #The first term is just conjugate kernel\n",
    "    KNTK = cross(Xs[L])\n",
    "    #print('3: ',KNTK)\n",
    "    for l in range(1,L+1):#l counts layers going forward from 1...\n",
    "        #we are going to construct terms that look like ( S^T S ) * (X^T X)\n",
    "        XtX = cross(Xs[l-1])\n",
    "        S = np.zeros((d_int[l],n),dtype=np.float32)\n",
    "        for i in range(n):\n",
    "            \n",
    "            #this is always the same, could be calculated once and saved\n",
    "            s = Ws[-1].T.reshape(-1)/np.sqrt(d_array[L]) #has shape input to last layer.\n",
    "\n",
    "            for k in range(L,l-1,-1): #counts backwards from L to l, inclusive both\n",
    "                s = Ds[k][:,i]*s\n",
    "                if k > l:\n",
    "                    s = np.dot(s,Ws[k-1])/np.sqrt(d_array[k-1])\n",
    "            S[:,i] = s\n",
    "        #print(l, cross(S) * XtX)\n",
    "        print(l, cross(S) * XtX)\n",
    "        KNTK += cross(S) * XtX\n",
    "    return KNTK\n",
    "\n",
    "def compute_NTK_new(Ws, Xs, d_int, d_array):\n",
    "    L = len(Xs)-1 #number of layers, Xs goes from inputs to right before outputs; X_0 is the input, X_L CK\n",
    "\n",
    "    n = Xs[0].shape[1] #number of datapoints\n",
    "\n",
    "    #holds the derivatives, first value is empty list...?; just a spacer, replace with array\n",
    "    Ds = [np.array([[0.0]],dtype=np.float32)] \n",
    "    for l in range(L):\n",
    "        Ds.append(d_activation(np.dot(Ws[l],Xs[l])))\n",
    "    print('length Ds: ',len(Ds))\n",
    "    #The first term is just conjugate kernel\n",
    "    KNTK = cross(Xs[L])\n",
    "\n",
    "    ####################\n",
    "    for l in range(1,L+1):#l counts layers going forward from 1...\n",
    "        #we are going to construct terms that look like ( S^T S ) * (X^T X)\n",
    "        XtX = cross(Xs[l-1])\n",
    "        S = np.expand_dims(Ws[-1].T.reshape(-1)/np.sqrt(d_array[L]),axis=1) #has shape input to last layer.\n",
    "        print('l : ',l)\n",
    "        for k in range(L,l-1,-1): #counts backwards from l\n",
    "            print('k : ',k)\n",
    "            S = Ds[k]*S\n",
    "            if k > l:\n",
    "                S = np.dot(S.T,Ws[k-1]).T/np.sqrt(d_array[k-1])\n",
    "        KNTK += cross(S) * XtX\n",
    "    return KNTK\n",
    "\n",
    "\n",
    "# L, should be 3\n",
    "\n",
    "# l :  1\n",
    "# k :  3\n",
    "# k :  2\n",
    "# k :  1\n",
    "\n",
    "# l :  2\n",
    "# k :  3\n",
    "# k :  2\n",
    "\n",
    "# l :  3\n",
    "# k :  3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc0c4f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NTK_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        print(m.weight.shape)\n",
    "        nn.init.normal_(m.weight.data)#/m.weight.shape[0]\n",
    "        if m.bias != None:\n",
    "            nn.init.normal_(m.bias.data)#/m.weight.shape[0]\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        print(m.weight.shape)\n",
    "        nn.init.normal_(m.weight.data)#/m.weight.shape[0]\n",
    "        if m.bias != None:\n",
    "            nn.init.normal_(m.bias.data)#/m.weight.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ddd4e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layerwise Needs each conjugate Kernel\n",
    "class dumb_small_layerwise(torch.nn.Module):\n",
    "    '''\n",
    "    simple network for test cases\n",
    "    '''\n",
    "    def __init__(self,):\n",
    "        super(dumb_small_layerwise, self).__init__()\n",
    "        \n",
    "        self.d1 = torch.nn.Linear(5,256,bias=False)\n",
    "        self.d2 = torch.nn.Linear(256,256,bias=False)\n",
    "        self.d3 = torch.nn.Linear(256,256,bias=False)\n",
    "        self.d4 = torch.nn.Linear(256,1,bias=False)\n",
    "        \n",
    "    def forward(self, x_0):\n",
    "        x_1 = activation(self.d1(x_0)) / np.sqrt(256)\n",
    "        x_2 = activation(self.d2(x_1)) / np.sqrt(256)\n",
    "        x_3 = activation(self.d3(x_2)) / np.sqrt(256)\n",
    "        x_4 = self.d4(x_3)\n",
    "        \n",
    "        return x_4, x_3, x_2, x_1, x_0\n",
    "    \n",
    "# Easy NTK expects one output alone\n",
    "class dumb_small(torch.nn.Module):\n",
    "    '''\n",
    "    simple network for test cases\n",
    "    '''\n",
    "    def __init__(self,):\n",
    "        super(dumb_small, self).__init__()\n",
    "        \n",
    "        self.d1 = torch.nn.Linear(5,256,bias=False)\n",
    "        self.d2 = torch.nn.Linear(256,256,bias=False)\n",
    "        self.d3 = torch.nn.Linear(256,256,bias=False)\n",
    "        self.d4 = torch.nn.Linear(256,1,bias=False)\n",
    "        \n",
    "    def forward(self, x_0):\n",
    "        x_1 = activation(self.d1(x_0)) / np.sqrt(256)\n",
    "        x_2 = activation(self.d2(x_1)) / np.sqrt(256)\n",
    "        x_3 = activation(self.d3(x_2)) / np.sqrt(256)\n",
    "        x_4 = self.d4(x_3)\n",
    "        return x_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b7f4757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 5])\n",
      "torch.Size([256, 256])\n",
      "torch.Size([256, 256])\n",
      "torch.Size([1, 256])\n",
      "torch.Size([256, 5])\n",
      "torch.Size([256, 256])\n",
      "torch.Size([256, 256])\n",
      "torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "device='cpu'\n",
    "\n",
    "model_small = dumb_small()\n",
    "model_small.to(device)\n",
    "model_small.apply(NTK_weights)\n",
    "\n",
    "#Reset the seed and \n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "model_layerwise = dumb_small_layerwise()\n",
    "model_layerwise.to(device)\n",
    "model_layerwise.apply(NTK_weights)\n",
    "\n",
    "x_test = np.random.normal(0,1,(10,5)).astype(np.float32)\n",
    "x_test = torch.from_numpy(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e053b37",
   "metadata": {},
   "source": [
    "# Now compute 3 ways:\n",
    "\n",
    "#### by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d537b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W1 = model_small.d1.weight.detach().numpy().T\n",
    "# W2 = model_small.d2.weight.detach().numpy().T\n",
    "\n",
    "# print(W1.shape)\n",
    "\n",
    "# A00 = W1[0,0] \n",
    "# A01 = W1[0,1]\n",
    "# A10 = W1[1,0] \n",
    "# A11 = W1[1,1]\n",
    "# A20 = W1[2,0]\n",
    "# A21 = W1[2,1]\n",
    "\n",
    "# B00 = W2[0,0]\n",
    "# B10 = W2[1,0]\n",
    "\n",
    "# X00 = x_test[0,0]\n",
    "# X01 = x_test[0,1]\n",
    "# X02 = x_test[0,2]\n",
    "# X10 = x_test[1,0]\n",
    "# X11 = x_test[1,1]\n",
    "# X12 = x_test[1,2]\n",
    "\n",
    "# J = np.array([[X00*B00, X01*B00, X02*B00, X00*B10, X01*B10, X02*B10, X00*A00 + X01*A10 + X02*A20, X00*A01 + X01*A11 + X02*A21],\n",
    "#               [X10*B00, X11*B00, X12*B00, X10*B10, X11*B10, X12*B10, X10*A00 + X11*A10 + X12*A20, X10*A01 + X11*A11 + X12*A21]])\n",
    "\n",
    "# J = J / np.sqrt(2) #in this 2 layer linear network, this is okay.\n",
    "\n",
    "# NTK_byhand = (J @ J.T) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26eaf1d",
   "metadata": {},
   "source": [
    "#### Layerwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50ee09bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_4, x_3, x_2, x_1, x_0 = model_layerwise(x_test)\n",
    "\n",
    "Ws = []\n",
    "Ws.append(model_layerwise.d1.weight.detach().numpy().astype(np.float32))\n",
    "Ws.append(model_layerwise.d2.weight.detach().numpy().astype(np.float32))\n",
    "Ws.append(model_layerwise.d3.weight.detach().numpy().astype(np.float32))\n",
    "Ws.append(model_layerwise.d4.weight.detach().numpy().astype(np.float32))\n",
    "\n",
    "Xs = [] # Xs are shape (output x #DP) ; however, typical python notation is reversed, so we take transpose here\n",
    "Xs.append(x_0.detach().numpy().T.astype(np.float32))\n",
    "Xs.append(x_1.detach().numpy().T.astype(np.float32))\n",
    "Xs.append(x_2.detach().numpy().T.astype(np.float32))\n",
    "Xs.append(x_3.detach().numpy().T.astype(np.float32))\n",
    "\n",
    "ds_int = []\n",
    "ds_int.append(0)\n",
    "ds_int.append(256)\n",
    "ds_int.append(256)\n",
    "ds_int.append(256)\n",
    "\n",
    "ds_array = []\n",
    "ds_array.append(np.array([0.0],dtype=np.float32)) #first element is the input length\n",
    "ds_array.append(np.array([256.0],dtype=np.float32))\n",
    "ds_array.append(np.array([256.0],dtype=np.float32)) #the remaining elements are the output lengths, but omit the last output length assumed 1.\n",
    "ds_array.append(np.array([256.0],dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85b5c220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [[ 5.6825566  -5.430897    5.690123   -0.83005065 -2.9812381   0.22461934\n",
      "  -0.20998777 -0.6562718  -1.2542443  -2.0309448 ]\n",
      " [-5.430897    9.94341    -7.792306    3.028438    4.137024    2.0289896\n",
      "   1.4452263  -1.9200761  -0.01225542  0.09289722]\n",
      " [ 5.690123   -7.792306    8.667182   -0.35607603 -3.753958    0.28389212\n",
      "  -0.4474241   2.6394753   1.309752   -1.47562   ]\n",
      " [-0.83005065  3.028438   -0.35607603  2.573817    0.84052926  2.0713265\n",
      "   1.1005363   0.55091065  1.2269455  -0.8878967 ]\n",
      " [-2.9812381   4.137024   -3.753958    0.84052926  4.8143535   0.1218503\n",
      "  -1.468686    0.4912028  -0.6373993   3.7537062 ]\n",
      " [ 0.22461934  2.0289896   0.28389212  2.0713265   0.1218503   1.8730148\n",
      "   1.1329446  -0.13625196  0.56144756 -1.3770355 ]\n",
      " [-0.20998777  1.4452263  -0.4474241   1.1005363  -1.468686    1.1329446\n",
      "   2.4877617  -1.762147   -0.51036155 -1.7449169 ]\n",
      " [-0.6562718  -1.9200761   2.6394753   0.55091065  0.4912028  -0.13625196\n",
      "  -1.762147    5.044641    4.0123224   1.0337812 ]\n",
      " [-1.2542443  -0.01225542  1.309752    1.2269455  -0.6373993   0.56144756\n",
      "  -0.51036155  4.0123224   4.6515493  -1.5123466 ]\n",
      " [-2.0309448   0.09289722 -1.47562    -0.8878967   3.7537062  -1.3770355\n",
      "  -1.7449169   1.0337812  -1.5123466   5.744681  ]]\n",
      "2 [[ 5.3935413  -5.1250367   5.4566307  -0.6772683  -2.7335477   0.31177336\n",
      "   0.09092138 -0.8805327  -1.6129485  -1.5370331 ]\n",
      " [-5.1250367   9.603936   -7.550323    2.8585858   3.5514736   1.9536947\n",
      "   1.3387681  -1.8084297   0.43464664 -0.76180434]\n",
      " [ 5.4566307  -7.550323    8.589415   -0.13548672 -3.326075    0.41374063\n",
      "  -0.23634712  2.5550532   0.92821157 -0.7398823 ]\n",
      " [-0.6772683   2.8585858  -0.13548672  2.5885286   0.628207    2.1087162\n",
      "   1.141978    0.60826945  1.3484403  -1.1135705 ]\n",
      " [-2.7335477   3.5514736  -3.326075    0.628207    4.803001   -0.06468324\n",
      "  -1.8123314   0.8433263  -0.5074279   3.9324684 ]\n",
      " [ 0.31177336  1.9536947   0.41374063  2.1087162  -0.06468324  1.9258909\n",
      "   1.2449725  -0.15874626  0.6134257  -1.5615305 ]\n",
      " [ 0.09092138  1.3387681  -0.23634712  1.141978   -1.8123314   1.2449725\n",
      "   2.6222472  -1.915299   -0.4784429  -2.232421  ]\n",
      " [-0.8805327  -1.8084297   2.5550532   0.60826945  0.8433263  -0.15874626\n",
      "  -1.915299    5.293977    4.0772233   1.4513685 ]\n",
      " [-1.6129485   0.43464664  0.92821157  1.3484403  -0.5074279   0.6134257\n",
      "  -0.4784429   4.0772233   4.8589206  -1.5893613 ]\n",
      " [-1.5370331  -0.76180434 -0.7398823  -1.1135705   3.9324684  -1.5615305\n",
      "  -2.232421    1.4513685  -1.5893613   6.280384  ]]\n",
      "3 [[ 5.323754   -4.6002965   5.11631    -0.49018905 -2.5710423   0.49667487\n",
      "   0.20228584 -1.1644864  -1.7572638  -1.6858201 ]\n",
      " [-4.6002965   8.827477   -6.753142    2.7925813   2.6451075   1.992328\n",
      "   1.6752055  -1.8003495   0.6326559  -1.4268497 ]\n",
      " [ 5.11631    -6.753142    7.9070153   0.02039068 -2.7507977   0.51037174\n",
      "  -0.41073343  2.4347968   0.8573508  -0.60715836]\n",
      " [-0.49018905  2.7925813   0.02039068  2.6349332   0.2942638   2.2001555\n",
      "   1.2673672   0.54446054  1.4869393  -1.5904162 ]\n",
      " [-2.5710423   2.6451075  -2.7507977   0.2942638   3.7650259  -0.33219248\n",
      "  -1.6481359   1.1574204   0.08785951  3.2186959 ]\n",
      " [ 0.49667487  1.992328    0.51037174  2.2001555  -0.33219248  2.0619035\n",
      "   1.3988545  -0.30083093  0.67052287 -2.0098057 ]\n",
      " [ 0.20228584  1.6752055  -0.41073343  1.2673672  -1.6481359   1.3988545\n",
      "   2.609501   -2.1570494  -0.6500741  -2.3689823 ]\n",
      " [-1.1644864  -1.8003495   2.4347968   0.54446054  1.1574204  -0.30083093\n",
      "  -2.1570494   5.624163    4.224232    1.8769903 ]\n",
      " [-1.7572638   0.6326559   0.8573508   1.4869393   0.08785951  0.67052287\n",
      "  -0.6500741   4.224232    4.673753   -0.9089658 ]\n",
      " [-1.6858201  -1.4268497  -0.60715836 -1.5904162   3.2186959  -2.0098057\n",
      "  -2.3689823   1.8769903  -0.9089658   5.761271  ]]\n"
     ]
    }
   ],
   "source": [
    "NTK_layerwise = compute_NTK(List(Ws), List(Xs), List(ds_int), List(ds_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f21ab90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length Ds:  4\n",
      "l :  1\n",
      "k :  3\n",
      "k :  2\n",
      "k :  1\n",
      "l :  2\n",
      "k :  3\n",
      "k :  2\n",
      "l :  3\n",
      "k :  3\n"
     ]
    }
   ],
   "source": [
    "NTK_layerwise_new = compute_NTK_new(List(Ws), List(Xs), List(ds_int), List(ds_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af9b2e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 21.9125   , -19.73215  ,  21.335274 ,  -2.494825 , -11.171752 ,\n",
       "          1.6160681,   0.7629754,  -4.5374703,  -6.928875 ,  -7.104703 ],\n",
       "       [-19.73215  ,  36.60446  , -28.594894 ,  11.1278   ,  12.858362 ,\n",
       "          7.630098 ,   5.757512 ,  -6.8720016,   2.0185657,  -3.425946 ],\n",
       "       [ 21.335274 , -28.594894 ,  32.979218 ,  -0.2559224, -12.720112 ,\n",
       "          1.9158003,  -1.0168245,   9.647783 ,   3.6374817,  -3.4760442],\n",
       "       [ -2.494825 ,  11.1278   ,  -0.2559224,  10.300727 ,   1.8668735,\n",
       "          8.448566 ,   4.7998934,   2.3872528,   5.6345854,  -5.111598 ],\n",
       "       [-11.171752 ,  12.858362 , -12.720112 ,   1.8668735,  16.888258 ,\n",
       "         -0.8336451,  -6.650405 ,   3.9538937,  -0.4944474,  13.900022 ],\n",
       "       [  1.6160681,   7.630098 ,   1.9158003,   8.448566 ,  -0.8336451,\n",
       "          7.82065  ,   5.2693157,  -0.9256127,   2.445969 ,  -6.921986 ],\n",
       "       [  0.7629754,   5.757512 ,  -1.0168245,   4.7998934,  -6.650405 ,\n",
       "          5.2693157,  10.380568 ,  -8.131702 ,  -2.6072369,  -8.616311 ],\n",
       "       [ -4.5374703,  -6.8720016,   9.647783 ,   2.3872528,   3.9538937,\n",
       "         -0.9256127,  -8.131702 ,  22.106916 ,  17.110071 ,   6.397989 ],\n",
       "       [ -6.928875 ,   2.0185657,   3.6374817,   5.6345854,  -0.4944474,\n",
       "          2.445969 ,  -2.6072369,  17.110071 ,  19.32481  ,  -4.5510883],\n",
       "       [ -7.104703 ,  -3.425946 ,  -3.4760442,  -5.111598 ,  13.900022 ,\n",
       "         -6.921986 ,  -8.616311 ,   6.397989 ,  -4.5510883,  23.205606 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NTK_layerwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac2877d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%lprun -f compute_NTK_new compute_NTK_new(List(Ws), List(Xs), List(ds_int), List(ds_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2bde5042",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit \n",
    "#compute_NTK_new(List(Ws), List(Xs), List(ds_int), List(ds_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e110dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "#compute_NTK(List(Ws), List(Xs), List(ds_int), List(ds_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5817fbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%memit compute_NTK(List(Ws), List(Xs), List(ds_int), List(ds_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80337490",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%memit compute_NTK_new(List(Ws), List(Xs), List(ds_int), List(ds_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800c0794",
   "metadata": {},
   "source": [
    "#### Jacobian autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1aea495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NTK_easy = calculate_NTK(model_small,x_test).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860ab7a5",
   "metadata": {},
   "source": [
    "# Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a2a2fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(NTK_byhand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db163930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 21.912504   -19.732151    21.33528     -2.4948232  -11.171747\n",
      "    1.6160682    0.76297545  -4.537472    -6.9288745   -7.104703  ]\n",
      " [-19.732151    36.604454   -28.594902    11.127797    12.858372\n",
      "    7.6300993    5.7575135   -6.872        2.0185654   -3.4259481 ]\n",
      " [ 21.33528    -28.594902    32.979202    -0.25592268 -12.72011\n",
      "    1.9158006   -1.016825     9.647791     3.6374834   -3.476046  ]\n",
      " [ -2.4948232   11.127797    -0.25592268  10.300735     1.8668748\n",
      "    8.44857      4.7998943    2.3872535    5.634585    -5.1116004 ]\n",
      " [-11.171747    12.858372   -12.72011      1.8668748   16.888258\n",
      "   -0.833645    -6.650404     3.9538946   -0.49444753  13.900022  ]\n",
      " [  1.6160682    7.6300993    1.9158006    8.44857     -0.833645\n",
      "    7.8206496    5.269317    -0.9256128    2.4459682   -6.9219923 ]\n",
      " [  0.76297545   5.7575135   -1.016825     4.7998943   -6.650404\n",
      "    5.269317    10.380568    -8.1317005   -2.6072366   -8.616312  ]\n",
      " [ -4.537472    -6.872        9.647791     2.3872535    3.9538946\n",
      "   -0.9256128   -8.1317005   22.106918    17.110075     6.39799   ]\n",
      " [ -6.9288745    2.0185654    3.6374834    5.634585    -0.49444753\n",
      "    2.4459682   -2.6072366   17.110075    19.324797    -4.5510917 ]\n",
      " [ -7.104703    -3.4259481   -3.476046    -5.1116004   13.900022\n",
      "   -6.9219923   -8.616312     6.39799     -4.5510917   23.205603  ]]\n"
     ]
    }
   ],
   "source": [
    "print(NTK_easy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1cbcc08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 21.9125    -19.73215    21.335274   -2.494825  -11.171752    1.6160681\n",
      "    0.7629754  -4.5374703  -6.928875   -7.104703 ]\n",
      " [-19.73215    36.60446   -28.594894   11.1278     12.858362    7.630098\n",
      "    5.757512   -6.8720016   2.0185657  -3.425946 ]\n",
      " [ 21.335274  -28.594894   32.979218   -0.2559224 -12.720112    1.9158003\n",
      "   -1.0168245   9.647783    3.6374817  -3.4760442]\n",
      " [ -2.494825   11.1278     -0.2559224  10.300727    1.8668735   8.448566\n",
      "    4.7998934   2.3872528   5.6345854  -5.111598 ]\n",
      " [-11.171752   12.858362  -12.720112    1.8668735  16.888258   -0.8336451\n",
      "   -6.650405    3.9538937  -0.4944474  13.900022 ]\n",
      " [  1.6160681   7.630098    1.9158003   8.448566   -0.8336451   7.82065\n",
      "    5.2693157  -0.9256127   2.445969   -6.921986 ]\n",
      " [  0.7629754   5.757512   -1.0168245   4.7998934  -6.650405    5.2693157\n",
      "   10.380568   -8.131702   -2.6072369  -8.616311 ]\n",
      " [ -4.5374703  -6.8720016   9.647783    2.3872528   3.9538937  -0.9256127\n",
      "   -8.131702   22.106916   17.110071    6.397989 ]\n",
      " [ -6.928875    2.0185657   3.6374817   5.6345854  -0.4944474   2.445969\n",
      "   -2.6072369  17.110071   19.32481    -4.5510883]\n",
      " [ -7.104703   -3.425946   -3.4760442  -5.111598   13.900022   -6.921986\n",
      "   -8.616311    6.397989   -4.5510883  23.205606 ]]\n"
     ]
    }
   ],
   "source": [
    "print(NTK_layerwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bae0c3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 21.912498  , -19.73215   ,  21.335274  ,  -2.494825  ,\n",
       "        -11.171751  ,   1.6160681 ,   0.7629754 ,  -4.53747   ,\n",
       "         -6.928875  ,  -7.104703  ],\n",
       "       [-19.73215   ,  36.60446   , -28.594893  ,  11.1278    ,\n",
       "         12.858361  ,   7.6300974 ,   5.757511  ,  -6.8720016 ,\n",
       "          2.0185657 ,  -3.425946  ],\n",
       "       [ 21.335274  , -28.594893  ,  32.979218  ,  -0.25592238,\n",
       "        -12.72011   ,   1.9158003 ,  -1.0168244 ,   9.647781  ,\n",
       "          3.6374817 ,  -3.4760442 ],\n",
       "       [ -2.494825  ,  11.1278    ,  -0.25592238,  10.300726  ,\n",
       "          1.8668733 ,   8.448566  ,   4.7998934 ,   2.3872526 ,\n",
       "          5.6345854 ,  -5.111598  ],\n",
       "       [-11.171751  ,  12.858361  , -12.72011   ,   1.8668733 ,\n",
       "         16.888256  ,  -0.8336451 ,  -6.650404  ,   3.9538934 ,\n",
       "         -0.4944474 ,  13.900022  ],\n",
       "       [  1.6160681 ,   7.6300974 ,   1.9158003 ,   8.448566  ,\n",
       "         -0.8336451 ,   7.8206496 ,   5.2693157 ,  -0.9256127 ,\n",
       "          2.445969  ,  -6.921986  ],\n",
       "       [  0.7629754 ,   5.757511  ,  -1.0168244 ,   4.7998934 ,\n",
       "         -6.650404  ,   5.2693157 ,  10.380568  ,  -8.131701  ,\n",
       "         -2.6072369 ,  -8.616311  ],\n",
       "       [ -4.53747   ,  -6.8720016 ,   9.647781  ,   2.3872526 ,\n",
       "          3.9538934 ,  -0.9256127 ,  -8.131701  ,  22.106915  ,\n",
       "         17.110071  ,   6.397989  ],\n",
       "       [ -6.928875  ,   2.0185657 ,   3.6374817 ,   5.6345854 ,\n",
       "         -0.4944474 ,   2.445969  ,  -2.6072369 ,  17.110071  ,\n",
       "         19.32481   ,  -4.5510883 ],\n",
       "       [ -7.104703  ,  -3.425946  ,  -3.4760442 ,  -5.111598  ,\n",
       "         13.900022  ,  -6.921986  ,  -8.616311  ,   6.397989  ,\n",
       "         -4.5510883 ,  23.205606  ]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NTK_layerwise_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49921d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!!! success!\n",
    "np.allclose(NTK_layerwise_new,NTK_easy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dad52e",
   "metadata": {},
   "source": [
    "# Can we get pytorch to give us the matrices with the correct kinds of derivative?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248a46a2",
   "metadata": {},
   "source": [
    "# verify hypothesis by increasing number of terms...\n",
    "\n",
    "### also lets us see how this fares on more than one datapoint... it'll be alot of calls to backward...\n",
    "\n",
    "### https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html\n",
    "\n",
    "### note to future self, since these are partials pretty sure we can use inputs = w1 and that will make it so the gradients are computed alittle more efficiently\n",
    "\n",
    "### and if inputs does work like that, then the reallly best thing would be to find a way to get vector gradients working.\n",
    "\n",
    "\n",
    "### also note pytorch doesn't support autograd on non-scalar values, which means we are forced to call on backwards multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0e89e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_layerwise.zero_grad()\n",
    "x_4, x_3, x_2, x_1, x_0 = model_layerwise(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a46bacb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_layerwise.zero_grad()\n",
    "x_4 = model_small(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e1448c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this method agrees between model layerwise and model small; meaning that the calculation is indepdent of those\n",
    "#two models. the insinuation is somehting is wrong with both my methods for calculating,--- the same thing, since\n",
    "#they agree with one another.\n",
    "\n",
    "#in the future we would iterate over layers instead of like this...\n",
    "layer_components_w1 = [] #and we can make these arrays since we will know their size and slice into them\n",
    "# and we would only do this one array element at a time.\n",
    "layer_components_w2 = []\n",
    "layer_components_w3 = []\n",
    "layer_components_w4 = []\n",
    "for output in x_4:\n",
    "#    model_layerwise.zero_grad()\n",
    "    model_small.zero_grad()\n",
    "    \n",
    "    output.backward(retain_graph=True)\n",
    "\n",
    "#     w3_grad = model_layerwise.d3.weight.grad.detach().numpy()\n",
    "#     w2_grad = model_layerwise.d2.weight.grad.detach().numpy()\n",
    "#     w1_grad = model_layerwise.d1.weight.grad.detach().numpy()\n",
    "    w4_grad = model_small.d4.weight.grad.detach().numpy()\n",
    "    w3_grad = model_small.d3.weight.grad.detach().numpy()\n",
    "    w2_grad = model_small.d2.weight.grad.detach().numpy()\n",
    "    w1_grad = model_small.d1.weight.grad.detach().numpy()\n",
    "\n",
    "\n",
    "    layer_components_w1.append(w1_grad.reshape(-1).copy())\n",
    "    layer_components_w2.append(w2_grad.reshape(-1).copy())\n",
    "    layer_components_w3.append(w3_grad.reshape(-1).copy())\n",
    "    layer_components_w4.append(w4_grad.reshape(-1).copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42b817fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_components_w1 = np.array(layer_components_w1)\n",
    "layer_components_w2 = np.array(layer_components_w2)\n",
    "layer_components_w3 = np.array(layer_components_w3)\n",
    "layer_components_w4 = np.array(layer_components_w4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a0cc4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "autograd_NTK = layer_components_w1 @ layer_components_w1.T+\\\n",
    "    layer_components_w2 @ layer_components_w2.T+\\\n",
    "    layer_components_w3 @ layer_components_w3.T+\\\n",
    "    layer_components_w4 @ layer_components_w4.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "770431ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.26212782, -0.13915719,  0.17513433, -0.01963228, -0.10402071,\n",
       "         0.02361404,  0.00735725, -0.01632607, -0.04401862, -0.04432306],\n",
       "       [-0.13915719,  0.26258352, -0.1740298 ,  0.08608555,  0.09840427,\n",
       "         0.05940237,  0.03724238, -0.04245349,  0.005094  , -0.00354181],\n",
       "       [ 0.17513433, -0.1740298 ,  0.25934973,  0.00085505, -0.08736758,\n",
       "         0.02161472, -0.00594713,  0.05563299,  0.01757241, -0.01121507],\n",
       "       [-0.01963228,  0.08608555,  0.00085505,  0.2057347 ,  0.03086133,\n",
       "         0.17467298,  0.06747642,  0.02441898,  0.06299526, -0.04137547],\n",
       "       [-0.10402071,  0.09840427, -0.08736758,  0.03086133,  0.27646452,\n",
       "        -0.00478765, -0.08475629,  0.03604071, -0.00832839,  0.12688056],\n",
       "       [ 0.02361404,  0.05940237,  0.02161472,  0.17467298, -0.00478765,\n",
       "         0.1864023 ,  0.08384367, -0.00451578,  0.03314169, -0.06775427],\n",
       "       [ 0.00735725,  0.03724238, -0.00594713,  0.06747642, -0.08475629,\n",
       "         0.08384367,  0.18186687, -0.08083846, -0.02090837, -0.0834853 ],\n",
       "       [-0.01632607, -0.04245349,  0.05563299,  0.02441898,  0.03604071,\n",
       "        -0.00451578, -0.08083846,  0.23185521,  0.15904889,  0.04506612],\n",
       "       [-0.04401862,  0.005094  ,  0.01757241,  0.06299526, -0.00832839,\n",
       "         0.03314169, -0.02090837,  0.15904889,  0.22880341, -0.02454299],\n",
       "       [-0.04432306, -0.00354181, -0.01121507, -0.04137547,  0.12688056,\n",
       "        -0.06775427, -0.0834853 ,  0.04506612, -0.02454299,  0.21056433]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_components_w2 @ layer_components_w2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eedfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 [[ 0.23109226 -0.13013916  0.17527533 -0.01816671 -0.09664266  0.02248176\n",
    "#    0.00669326 -0.01512574 -0.04260881 -0.04601773]\n",
    "#  [-0.13013916  0.24945484 -0.17660742  0.08622429  0.10080978  0.06020794\n",
    "#    0.03772704 -0.04323682  0.00542523 -0.00371261]\n",
    "#  [ 0.17527533 -0.17660742  0.27889174  0.00089781 -0.09548703  0.02274703\n",
    "#   -0.00634935  0.0582776   0.01933318 -0.01291431]\n",
    "#  [-0.01816671  0.08622429  0.00089781  0.22381395  0.03286307  0.18501176\n",
    "#    0.06594371  0.02559525  0.07113553 -0.0444255 ]\n",
    "#  [-0.09664266  0.10080978 -0.09548703  0.03286307  0.28040892 -0.00513606\n",
    "#   -0.08478455  0.03666005 -0.00888365  0.14024408]\n",
    "#  [ 0.02248176  0.06020794  0.02274703  0.18501176 -0.00513606  0.19367565\n",
    "#    0.08173902 -0.00470114  0.03726654 -0.07162631]\n",
    "#  [ 0.00669326  0.03772704 -0.00634935  0.06594371 -0.08478455  0.08173902\n",
    "#    0.17753676 -0.08210288 -0.02242952 -0.0883496 ]\n",
    "#  [-0.01512574 -0.04323682  0.0582776   0.02559525  0.03666005 -0.00470114\n",
    "#   -0.08210288  0.22881114  0.16830118  0.04913284]\n",
    "#  [-0.04260881  0.00542523  0.01933318  0.07113553 -0.00888365  0.03726654\n",
    "#   -0.02242952  0.16830118  0.25831077 -0.02846634]\n",
    "#  [-0.04601773 -0.00371261 -0.01291431 -0.0444255   0.14024408 -0.07162631\n",
    "#   -0.0883496   0.04913284 -0.02846634  0.23442203]]\n",
    "\n",
    "# 1 [[ 4.3372995e-01 -1.2680553e-01  2.4192266e-01 -3.2461844e-02\n",
    "#   -9.3728907e-02  1.0109340e-02 -7.7008591e-03 -1.9981360e-02\n",
    "#   -5.2510254e-02 -6.3229941e-02]\n",
    "#  [-1.2680553e-01  4.2842412e-01 -2.2196750e-01  9.7295851e-02\n",
    "#    9.3750760e-02  6.9861203e-02  3.1264532e-02 -2.7425971e-02\n",
    "#   -2.4413578e-04  1.6981254e-03]\n",
    "#  [ 2.4192266e-01 -2.2196750e-01  5.7009739e-01 -1.2536406e-02\n",
    "#   -1.0858573e-01  1.0893183e-02 -1.3385277e-02  5.4588843e-02\n",
    "#    3.9020326e-02 -4.5721728e-02]\n",
    "#  [-3.2461844e-02  9.7295851e-02 -1.2536406e-02  3.0397519e-01\n",
    "#    3.8576838e-02  2.1457963e-01  6.2807687e-02  2.4017636e-02\n",
    "#    6.5343060e-02 -2.7964469e-02]\n",
    "#  [-9.3728907e-02  9.3750760e-02 -1.0858573e-01  3.8576838e-02\n",
    "#    3.9145073e-01  6.3925628e-03 -5.8467742e-02  1.3922420e-02\n",
    "#   -2.0138728e-02  1.5643328e-01]\n",
    "#  [ 1.0109340e-02  6.9861203e-02  1.0893183e-02  2.1457963e-01\n",
    "#    6.3925628e-03  2.1332595e-01  7.2265819e-02 -5.4552890e-03\n",
    "#    2.7105577e-02 -5.9520919e-02]\n",
    "#  [-7.7008591e-03  3.1264532e-02 -1.3385277e-02  6.2807687e-02\n",
    "#   -5.8467742e-02  7.2265819e-02  2.3293228e-01 -5.9420872e-02\n",
    "#   -1.6525010e-02 -7.4892543e-02]\n",
    "#  [-1.9981360e-02 -2.7425971e-02  5.4588843e-02  2.4017636e-02\n",
    "#    1.3922420e-02 -5.4552890e-03 -5.9420872e-02  3.5255811e-01\n",
    "#    2.0547493e-01  2.8280120e-02]\n",
    "#  [-5.2510254e-02 -2.4413578e-04  3.9020326e-02  6.5343060e-02\n",
    "#   -2.0138728e-02  2.7105577e-02 -1.6525010e-02  2.0547493e-01\n",
    "#    3.9798790e-01 -4.7951743e-02]\n",
    "#  [-6.3229941e-02  1.6981254e-03 -4.5721728e-02 -2.7964469e-02\n",
    "#    1.5643328e-01 -5.9520919e-02 -7.4892543e-02  2.8280120e-02\n",
    "#   -4.7951743e-02  3.7898111e-01]]\n",
    "\n",
    "# 3 [[ 0.2271919  -0.11885324  0.15557915  0.00355438 -0.09601458  0.04310704\n",
    "#    0.01562474 -0.02591271 -0.04562492 -0.06052731]\n",
    "#  [-0.11885324  0.21490423 -0.15661456  0.07115156  0.07242595  0.05466134\n",
    "#    0.04481084 -0.0430408   0.00233283 -0.02437924]\n",
    "#  [ 0.15557915 -0.15661456  0.23296475  0.02149654 -0.07168935  0.03008167\n",
    "#   -0.01506153  0.0704124   0.03533587 -0.00024379]\n",
    "#  [ 0.00355438  0.07115156  0.02149654  0.20458075  0.00798919  0.18973994\n",
    "#    0.08918232  0.03191677  0.07305919 -0.07019431]\n",
    "#  [-0.09601458  0.07242595 -0.07168935  0.00798919  0.20534858 -0.0244321\n",
    "#   -0.09154298  0.04552512  0.01293609  0.11445449]\n",
    "#  [ 0.04310704  0.05466134  0.03008167  0.18973994 -0.0244321   0.20804457\n",
    "#    0.1102321  -0.00291464  0.03953396 -0.1023953 ]\n",
    "#  [ 0.01562474  0.04481084 -0.01506153  0.08918232 -0.09154298  0.1102321\n",
    "#    0.19427639 -0.08135503 -0.02481991 -0.11032288]\n",
    "#  [-0.02591271 -0.0430408   0.0704124   0.03191677  0.04552512 -0.00291464\n",
    "#   -0.08135503  0.20784861  0.15843329  0.06904436]\n",
    "#  [-0.04562492  0.00233283  0.03533587  0.07305919  0.01293609  0.03953396\n",
    "#   -0.02481991  0.15843329  0.20318013  0.00192579]\n",
    "#  [-0.06052731 -0.02437924 -0.00024379 -0.07019431  0.11445449 -0.1023953\n",
    "#   -0.11032288  0.06904436  0.00192579  0.20982149]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81e25fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.2134769 , -0.52676934,  0.7556085 , -0.04334489, -0.4261384 ,\n",
       "         0.12994415,  0.05166331, -0.09718898, -0.19393545, -0.2207354 ],\n",
       "       [-0.52676934,  1.170739  , -0.6912442 ,  0.3135066 ,  0.30509868,\n",
       "         0.2326821 ,  0.16158278, -0.17250077,  0.01924271, -0.06059065],\n",
       "       [ 0.7556085 , -0.6912442 ,  1.360573  ,  0.05424047, -0.35490948,\n",
       "         0.11159445, -0.03115758,  0.2816458 ,  0.13358329, -0.04844324],\n",
       "       [-0.04334489,  0.3135066 ,  0.05424047,  0.85371536,  0.0809916 ,\n",
       "         0.74862355,  0.31674808,  0.1227646 ,  0.27802402, -0.20531508],\n",
       "       [-0.4261384 ,  0.30509868, -0.35490948,  0.0809916 ,  1.1267519 ,\n",
       "        -0.05709318, -0.35211247,  0.14757422,  0.00431177,  0.48566008],\n",
       "       [ 0.12994415,  0.2326821 ,  0.11159445,  0.74862355, -0.05709318,\n",
       "         0.8303773 ,  0.40138292, -0.0151794 ,  0.13875796, -0.32825872],\n",
       "       [ 0.05166331,  0.16158278, -0.03115758,  0.31674808, -0.35211247,\n",
       "         0.40138292,  0.8189769 , -0.32440397, -0.10751157, -0.354749  ],\n",
       "       [-0.09718898, -0.17250077,  0.2816458 ,  0.1227646 ,  0.14757422,\n",
       "        -0.0151794 , -0.32440397,  1.087006  ,  0.714474  ,  0.21162188],\n",
       "       [-0.19393545,  0.01924271,  0.13358329,  0.27802402,  0.00431177,\n",
       "         0.13875796, -0.10751157,  0.714474  ,  1.040845  , -0.06647566],\n",
       "       [-0.2207354 , -0.06059065, -0.04844324, -0.20531508,  0.48566008,\n",
       "        -0.32825872, -0.354749  ,  0.21162188, -0.06647566,  0.96402454]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autograd_NTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a815d024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!!!! SUCCESS!\n",
    "np.allclose(NTK_easy,autograd_NTK,atol=0.0000001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
